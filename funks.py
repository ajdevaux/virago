from future.builtins import input
import matplotlib.pyplot as plt
import glob, os, json, sys
import pandas as pd
import numpy as np
from skimage import exposure, feature, io, transform, filters, morphology
from math import sqrt, ceil
from scipy import stats
if int(sys.version[0]) == 3:
    import joypy
#*********************************************************************************************#
#*********************************************************************************************#
def IRIScsv_reader(chip_name, spot_data):
    min_corr = float(input("\nWhat is the correlation cutoff for particle count? (choose value between 0.5 and 1)\t"))
    contrast_window = str(input("\nEnter the minimum and maximum percent contrast values, separated by a comma (for VSV, 0-6% works well)\t"))
    contrast_window = contrast_window.split(",")
    min_corr_str = str("%.2F" % min_corr)
    particle_counts = ([])
    particle_dict = {}

    csv_list = sorted(glob.glob('*.csv'))
    for file in sorted(glob.glob('*.csv')): ##This pulls particle data from the CSVs generated by nanoViewer
        csvfile = file
        csv_info = csvfile.split(".")
        csv_data = pd.read_table(
                                 csvfile, sep = ',',
                                 error_bad_lines = False, usecols = [1,2,3,4,5],
                                 names = ("Contrast", "Correlation", "X", "Y", "Slice")
                                 )
        k, particles = 0,0
        corr2 = pd.DataFrame([0,0])
        for row in csv_data.iterrows():
            contrast = csv_data.get_value(k, 'Contrast')
            contrast = (contrast - 1) * 100
            corr = csv_data.get_value(k, 'Correlation')
            if float(contrast_window[0]) < contrast <= float(contrast_window[1]) and corr >= min_corr:
                filter_df = pd.DataFrame([contrast, corr]).T
                corr2 = pd.concat([corr2, filter_df], ignore_index = True)
                k += 1
                particles += 1
                continue
            else:
                k += 1
                continue

        csv_id = str(csv_info[1])+"."+str(csv_info[2])
        particle_dict[csv_id] = list(corr2.iloc[:,0])
        particle_counts.append(particles)
        print('File scanned:  '+ csvfile + '; Particles counted: ' + str(particles))
    particle_count_col = str('particle_count_'+ min_corr_str
                       + '_' + contrast_window[0]
                       + '_' + contrast_window[1]+ '_')
    spot_data[particle_count_col] = particle_counts
    for row in spot_data.iterrows():
        filtered_density = (spot_data.loc[:,particle_count_col]
                          / spot_data.loc[:,'area']) / 1000
    spot_data = pd.concat([spot_data, filtered_density.rename('kparticle_density')], axis = 1)
    dict_file = pd.io.json.dumps(particle_dict)
    f = open('../virago_output/' + chip_name + '_particle_dict_' + min_corr_str + 'corr.txt', 'w')
    f.write(dict_file)
    f.close()
    print("Dictionary file generated")

    return min_corr, spot_data, particle_dict, contrast_window
#*********************************************************************************************#
#*********************************************************************************************#
def joyplot(min_corr, chip_name, spot_dict, contrast_window):

    min_corr_str = str("%.2F" % min_corr)
    min_cont = float(contrast_window[0])
    max_cont = float(contrast_window[1])

    with open('../virago_output/'
        + chip_name + '_particle_dict_'
        + min_corr_str + 'corr.txt', 'r') as infile:
        particle_dict = json.load(infile)

    particle_df = pd.DataFrame.from_dict(particle_dict, orient = 'index').T
    particle_df = particle_df.sort_index(axis = 1, ascending = False)

    ##Joyplot generator
    spots_to_hist = str(input("Which spots would you like to generate joyplots for? (Enter all spots you want separated by a comma)\t"))
    spots_to_list = spots_to_hist.split(",")

    i = 0
    joydf = pd.DataFrame([])
    for col in particle_df.columns:
        spot_scan = particle_df.columns[i]
        hist_spot = int(spot_scan[:3])
        if str(hist_spot) in spots_to_hist:
            current_spot = int(spots_to_hist.index(str(hist_spot)))
            joydf = pd.concat([joydf, particle_df.iloc[:,i]], axis = 1)
            i += 1
            continue
        else:
            i += 1
            continue

    fig, ax = joypy.joyplot(joydf, kind = 'counts', bins = 60,
                            ylim = 'max', overlap = 1.25, figsize = (6,5),
                            fade = True, color = '#377eb8', linewidth = 0.5,
                            grid = True, x_range = (min_cont,max_cont), ylabelsize = 3)
    plt.ylabel("Particle Frequency")
    plt.xlabel("% Contrast", color = 'k')
    plt.title(chip_name + ": Particle Frequency Distributions above "+min_corr_str+" Correlation\n"
                        + "All Passes of "
                        + spot_dict[int(spots_to_hist[current_spot])]
                        + ' Spot ' + str(spots_to_hist[current_spot]))
    #plt.show()
    fig.savefig('../virago_output/' + chip_name +'_'
                + spot_dict[int(spots_to_hist[current_spot])]
                + '_' + str(spots_to_hist[current_spot]) + '_joyplot.png',
                bbox_inches = 'tight', pad_inches = 0.1, dpi = 300)
    print('File generated: ' + chip_name + '_' + spot_dict[int(spots_to_hist[current_spot])]
                             + '_Spot' + str(spots_to_hist[current_spot]) + '_joyplot.png')
    plt.clf()

#*********************************************************************************************#
# This function will scan PGMs (portable graymaps) generated by the IRIS instrument. It
# dynamically locates the antibody spot and will count blobs (either LoG or DoG algorithm)
# inside the perimeter of the spot.
# It then filters out blobs based on SDM of the background and by assigning them to bins to
# prevent counting the same particle across several images.
#*********************************************************************************************#

def IRISpgm_scanner(spot, scan, scan_list):
    total_particles = np.empty(shape = (0,6))
    for file in scan_list:
        perc_contrast, back_lum_sdm, zslice_list = [],[],[]
        pgmfile = file
        pgm_name = pgmfile.split(".")
        zslice = int(pgm_name[3])
        #scan_no = int(pgm_name[2])
        #spot_no = int(pgm_name[1])
        png = '.'.join(pgm_name[:3])
        pic = io.imread(pgmfile)
#---------------------------------------------------------------------------------------------#
# ANTIBODY SPOT DETECTION:
#---------------------------------------------------------------------------------------------#
        p1, p99 = np.percentile(pic, (1, 99))
        canny_sig = 2
        if p1 < 28000:
            p1 = 28000
            canny_sig = 4
        print(p1, p99, canny_sig)
        pic_hc = exposure.rescale_intensity(pic, in_range=(p1, p99))
        nrows, ncols = pic.shape
        row, col = np.ogrid[:nrows,:ncols]
        if zslice == 5:
            pic_hc_mid = exposure.rescale_intensity(pic, in_range=(p1, p99))
        if zslice == 1:
            print("Locating antibody spot...")
            pic_eq = exposure.equalize_adapthist(pic)
            pic_selem = filters.rank.equalize(pic_eq,selem = morphology.disk(50))
            pic_edge = feature.canny(pic_selem, sigma = canny_sig)
            hough_radius = np.arange(500, 600, 25)
            hough_res = transform.hough_circle(pic_edge, hough_radius)
            accums, cx, cy, rad = transform.hough_circle_peaks(hough_res, hough_radius,
                                                                 total_num_peaks=1)
            a = (row - cy)
            b = (col - cx)
            c = (rad - 20)
            outer_disk_mask = (a**2 + b**2 > c**2)
        pic_hc[outer_disk_mask] = pic_hc.max()
        pic[outer_disk_mask] = pic.max()
        pix_area = (outer_disk_mask == 0).sum()
        pix_sz_micron = 5.86
        mag = 40
        area_sqmm = ((pix_area * (pix_sz_micron)**2) / mag**2)*1e-6
#---------------------------------------------------------------------------------------------#
# BLOB DETECTION: Make threshold lower to detect more particles; make min_sigma lower to
#                 detect smaller particles
#---------------------------------------------------------------------------------------------#
        blobs = feature.blob_dog(
                                 pic_hc, min_sigma = 1, max_sigma = 2.5,
                                 threshold=.05, overlap = 0
                                 )
        blobs[:,2] = blobs[:,2]*sqrt(2)
        print(len(blobs))
        if len(blobs) == 0:
            blobs = np.asarray([[0,0,1],[20,20,1]], dtype=float)
#---------------------------------------------------------------------------------------------#
# SDM Background Filter: Removes blobs on high-contrast edges
#---------------------------------------------------------------------------------------------#
        i = 0
        for blob in blobs:
            y,x,r = blobs[i]
            y = int(y)
            x = int(x)
            r = int(ceil(r))
            point_lum = pic[y,x]
            if y >= (nrows - 5):
                back_lum_list = (pic[y-r, x-r], pic[y-r, x+r], pic[y-r, x],
                                 pic[y, x+r], pic[y, x-r])
            if x >= (ncols - 5):
                back_lum_list = (pic[y-r, x-r], pic[y+r, x-r],
                                 pic[y+r, x], pic[y-r, x], pic[y, x-r])
            else:
                back_lum_list = (pic[y-r, x-r], pic[y+r, x+r],
                                 pic[y-r, x+r], pic[y+r, x-r],
                                 pic[y+r, x],
                                 pic[y-r, x],
                                 pic[y, x+r], pic[y, x-r])
            back_lum_avg = np.mean(back_lum_list)
            back_lum_sdm_pt = np.std(back_lum_list)/sqrt(len(back_lum_list))
            perc_contrast_pt = ((point_lum - back_lum_avg) * 100) / back_lum_avg
            perc_contrast.append([perc_contrast_pt])
            back_lum_sdm.append([back_lum_sdm_pt])
            zslice_list.append([zslice])
            i += 1
        blobs = np.append(blobs, np.asarray(perc_contrast), axis = 1)
        blobs = np.append(blobs, np.asarray(back_lum_sdm), axis = 1)
        blobs = np.append(blobs, np.asarray(zslice_list), axis = 1)
        sdm_filter = 100 ###Make lower if edge particles are being detected
        ix = np.where(blobs[:,4] <= sdm_filter)
        particles = blobs[ix]
        print("\nImage scanned: " + str(pgmfile))

        if len(particles) == 0:
            particles = np.empty(shape = (0,6))
        #print(particles)
        total_particles = np.concatenate((total_particles, particles), axis = 0)
#---------------------------------------------------------------------------------------------#
    if len(total_particles) == 0:
        total_particles = np.concatenate((total_particles, np.zeros((1,6))), axis = 0)

    total_part_df = pd.DataFrame(total_particles)

    total_part_df.rename(columns = {0:'y', 1:'x', 2:'r', 3:'pc', 4:'sdm', 5:'z'},
                         inplace = True)

    y = total_part_df['y']
    x = total_part_df['x']
    pc = total_part_df['pc']
    yx = np.asarray(total_part_df.loc[:,'y':'x'])

    twoD_hist_particles, xedges, yedges, binno = stats.binned_statistic_2d(x,y,pc, statistic = 'max', bins = 130) ##Lower bin count if there are too many overlapping counts

    xbins = [(a + b) / 2 for a, b in zip(xedges[::1], xedges[1::])]
    ybins = [(a + b) / 2 for a, b in zip(yedges[::1], yedges[1::])]


    twoD_hist_particles = pd.DataFrame(twoD_hist_particles)
    twoD_hist_particles = twoD_hist_particles.T ##It needs to be transposed for some reason
    twoD_hist_particles.columns = [xbins]
    twoD_hist_particles.index = [ybins]

    uniq_particles = twoD_hist_particles.stack()
    uniq_particles = uniq_particles.reset_index(level = [0,1])
    uniq_particles.rename(columns = {'level_0':'y', 'level_1':'x', 0:'pc'}, inplace = True)
    round(uniq_particles.y,0); round(uniq_particles.x,0)
    uniq_particles.to_csv('../virago_output/'+ png +"uniq_particles.csv", sep = ",")

    particle_count = len(uniq_particles)
    print("Particles counted: " + str(particle_count))

    title = png + " Particles"
    fig, axes = plt.subplots(1,1)
    axes.set_title(title)
    axes.imshow(pic_hc_mid, interpolation = None, cmap = 'gray')
    axes.axis('off')
    y = uniq_particles['y']
    x = uniq_particles['x']
    pc = uniq_particles['pc']

    i = 0

    for index in uniq_particles.iterrows():
        point = plt.Circle((x[i], y[i]), pc[i] * 2,
                            color='#FFB2AD', linewidth=0.5,
                            fill=False, alpha = 0.65)
        spot = plt.Circle((cx, cy), rad, color='#5A81BB', linewidth=1, fill=False)
        axes.add_patch(point)
        axes.add_patch(spot)
        axes.set_axis_off
        i += 1
    plt.savefig('../virago_output/'+ png +'.png', dpi = 300)
    #plt.show()
    plt.close()

    return particle_count, uniq_particles, area_sqmm



def density_normalizer(spot_data, pass_counter):
    normalized_density = ([])
    m,p,q = 0,0,0
    for row in spot_data.iterrows():
        while p < len(spot_data.index):
            if q < (pass_counter) and p < (pass_counter):
                normalized_density.append(round(spot_data.loc[p,'kparticle_density']
                    - spot_data.loc[m,'kparticle_density'],2))
                p += 1
                q += 1
                continue
            elif q == (pass_counter):
                m += (pass_counter)
                normalized_density.append(spot_data.loc[p,'kparticle_density']
                    - spot_data.loc[m,'kparticle_density'])
                p += 1
                q = 1
                continue
            elif q < (pass_counter) and p >= (pass_counter):
                normalized_density.append(round(spot_data.loc[p,'kparticle_density']
                    - spot_data.loc[m,'kparticle_density'],2))
                q += 1
                p += 1
                continue
            else:
                continue
    return normalized_density
