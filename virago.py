#! /usr/local/bin/python3
from datetime import datetime
from future.builtins import input
from lxml import etree
#from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
#import matplotlib.image as mpimg
import matplotlib.mlab as mlab
import pandas as pd
import numpy as np
from scipy import stats
from skimage import exposure, feature, io, transform, filters
import glob, os, json, sys, math, warnings
# if int(sys.version[0]) == 3:
#     import joypy
#from funks import IRIScsv_reader, IRISpgm_scanner, joyplot, density_normalizer
pd.set_option('display.width', 1000)
pd.options.display.max_rows = 999
#*********************************************************************************************#
#
#           FUNCTIONS
#
#*********************************************************************************************#
#*********************************************************************************************#
# This function will scan PGMs (portable graymaps) generated by the IRIS instrument. It
# dynamically locates the antibody spot and will count blobs (either LoG or DoG algorithm)
# inside the perimeter of the spot.
# It then filters out blobs based on SDM of the background and by assigning them to bins to
# prevent counting the same particle across several images.
#*********************************************************************************************#
def image_details(fig1, fig2, fig3, pic_edge, dpi):
    bin_no = 55
    nrows, ncols = fig1.shape
    figsize = (ncols/dpi/2, nrows/dpi/2)
    fig = plt.figure(figsize = figsize, dpi = dpi)

    ax_img = plt.Axes(fig,[0,0,1,1])
    ax_img.set_axis_off()
    fig.add_axes(ax_img)

    #fig3_bins = len(set(fig3.ravel()))
    fig3[pic_edge] = fig3.max()*2

    ax_img.imshow(fig3, cmap = 'gray')

    pic_cdf1, cbins1 = exposure.cumulative_distribution(fig1, bin_no)
    pic_cdf2, cbins2 = exposure.cumulative_distribution(fig2, bin_no)
    pic_cdf3, cbins3 = exposure.cumulative_distribution(fig3, bin_no)
    ax_hist1 = plt.axes([.05, .05, .25, .25])
    ax_cdf1 = ax_hist1.twinx()
    ax_hist2 = plt.axes([.375, .05, .25, .25])
    ax_cdf2 = ax_hist2.twinx()
    ax_hist3 = plt.axes([.7, .05, .25, .25])
    ax_cdf3 = ax_hist3.twinx()

    pixels1, hbins1, patches1 = ax_hist1.hist(fig1.ravel(),bin_no, facecolor = 'r', normed = True)
    pixels2, hbins2, patches2 = ax_hist2.hist(fig2.ravel(), bin_no, facecolor = 'b', normed = True)
    pixels3, hbins3, patches3 = ax_hist3.hist(fig3.ravel(), bins = bin_no,
                                              facecolor = 'g', normed = True)

    ax_hist1.patch.set_alpha(0); ax_hist2.patch.set_alpha(0); ax_hist3.patch.set_alpha(0)

    ax_cdf1.plot(cbins1, pic_cdf1, color = 'w')
    ax_cdf2.plot(cbins2, pic_cdf2, color = 'c')
    ax_cdf3.plot(cbins3, pic_cdf3, color = 'y')
    ax_hist1.set_title("Normalized", color = 'r')
    ax_hist2.set_title("CLAHE Equalized", color = 'b')
    ax_hist3.set_title("Contrast Stretched", color = 'g')
    ax_hist1.set_ylim([0,max(pixels1)])
    ax_hist3.set_ylim([0,max(pixels3)])
    ax_hist1.set_xlim([np.median(fig1)-0.25,np.median(fig1)+0.25])
    #ax_cdf1.set_ylim([0,1])
    ax_hist2.set_xlim([np.median(fig2)-0.5,np.median(fig2)+0.5])
    ax_hist3.set_xlim([0,1])
    plt.show()
    #plt.savefig('../virago_output/' + chip_name + '/' + pgmfile + '_clahe_norm.png', dpi = dpi)
    plt.close('all')
    return hbins1, pic_cdf1
#*********************************************************************************************#
def IRISpgm_scanner(mirror_file, scan_list, image_detail_toggle):
    #print(scan_list)
    total_particles = np.empty(shape = (0,6))
    pic_dict = {}
    #chip_name_scan = 'pCHIP001.005.010'
    #pgm_list = sorted(glob.glob('*.pgm'))
    #pgm_list = [val for val in pgm_list if ".".join(val.split(".")[0:3]) == chip_name_scan]
    dpi = 96
    if not os.path.exists('../virago_output/'+ chip_name): os.makedirs('../virago_output/' + chip_name)

    try: mirror = io.imread(mirror_file)
    except FileNotFoundError: mirror_toggle = False; print("\nMirror file absent\n")
    else: mirror_toggle = True; print("\nMirror file detected\n")

    fluor_files = [file for file in scan_list if file.endswith('A.pgm' or 'B.pgm' or 'C.pgm')]
    if fluor_files:
        [scan_list.remove(file) for file in scan_list if file in fluor_files]
        print("Fluorescent channel detected\n")
        #print(fluor_files)
    for pgmfile in scan_list:

        pgm_name = pgmfile.split(".")
        #chip_name = str(pgm_name[0])
        zslice = int(pgm_name[3])
        png = '.'.join(pgm_name[:3])
        pic = io.imread(pgmfile)
        nrows, ncols = pic.shape
        row, col = np.ogrid[:nrows,:ncols]

        if mirror_toggle is True: pic = pic / mirror

        norm_scalar = np.median(pic) * 2
        pic_norm = pic / norm_scalar
        pic_norm[pic_norm > 1] = 1
#---------------------------------------------------------------------------------------------#
        # ANTIBODY SPOT DETECTION:
        # Decrease canny_sig if spots are not being detected accurately; increase if spot detection
        # takes too long.
        clahe_cliplim = .002#(pdf_size * 0.135) - 0.001
#---------------------------------------------------------------------------------------------#
        warnings.simplefilter("ignore", UserWarning)
        #print(clahe_cliplim)
        pic_clahe = exposure.equalize_adapthist(pic_norm, clip_limit = clahe_cliplim)

        p1,p2 = np.percentile(pic_clahe, (2, 98))
        pic_rescale = exposure.rescale_intensity(pic_clahe, in_range=(p1,p2))
        #pic_rescale2 = pic_rescale.copy()
        if zslice == 1:
            print("Locating antibody spot on " + pgmfile)
            canny_sig = 2#abs((pdf_size * -8.33) + 3.25)
            #print(canny_sig)
            #pic_clahe_1 = exposure.equalize_adapthist(pic, clip_limit = 0.4)
            pic_edge = feature.canny(pic_rescale, sigma = canny_sig)
            hough_radius = range(500, 601, 25)
            hough_res = transform.hough_circle(pic_edge, hough_radius)
            accums, cx, cy, rad = transform.hough_circle_peaks(hough_res, hough_radius,
                                                                 total_num_peaks=1)
            if cx < ncols * 0.25 or cx > ncols * 0.75:
                cx = ncols * 0.5
                cy = nrows * 0.5
                rad = rad * 0.5
            height = row - cy
            width = col - cx
            rad = rad - 50
            print(cx,cy,rad)
            outer_disk_mask = (width**2 + height**2 > rad**2)
        if zslice == np.ceil(zslice_count/2): pic_rescale_mid = pic_rescale.copy()

        if image_detail_toggle.lower() in ('yes', 'y'):
            hbins1, pic_cdf1 = image_details(pic_norm, pic_clahe, pic_rescale.copy(), pic_edge, dpi)
        else:
            pic_cdf1, cbins1 = exposure.cumulative_distribution(pic_norm, 55)
            pixels1, hbins1 = np.histogram(pic_norm.ravel(),55)
        cdf_range = hbins1[np.argwhere((pic_cdf1 < 0.95) & (pic_cdf1 > 0.05)).flatten()]
        # pdf_size = round(max(cdf_range) - min(cdf_range),2)
        # print("Histogram size: " + str(pdf_size))

        pic_rescale[outer_disk_mask] = pic_rescale.max()
        pic[outer_disk_mask] = pic.max()

        def border_masker(image):
            border_mask = 5
            image[0:border_mask,:], image[-(border_mask):,:] = image.max(), image.max()
            image[:,0:border_mask], image[:,-(border_mask):] = image.max(), image.max()
            return image

        pic = border_masker(pic)
        pic_rescale = border_masker(pic_rescale)

        pix_area = (pic != pic.max()).sum()
        pix_sz_micron = 5.86
        mag = 40
        if (nrows,ncols) == (1080,1072):
            pix_sz_micron = 3.45
            mag = 44

        area_sqmm = round(((pix_area * (pix_sz_micron)**2) / mag**2)*1e-6, 6)
        area_squm = int(area_sqmm * 1e6)
#---------------------------------------------------------------------------------------------#
        # BLOB DETECTION: Make threshold lower to detect more particles; make min_sigma lower to
        #                 detect smaller particles
        min_sig = 1; max_sig = 2.2; thresh = .05
        if mirror_toggle is False: min_sig = 0.75
#---------------------------------------------------------------------------------------------#
        def blob_detect(image, min_sig, max_sig, thresh):
            blobs = feature.blob_dog(
                                     image, min_sigma = min_sig, max_sigma = max_sig,
                                     threshold = thresh, overlap = 0
                                     ) ## Difference of Gaussian algorithm
            blobs[:,2] = blobs[:,2]*math.sqrt(2)
            if len(blobs) == 0:
                blobs = np.asarray([[0,0,1],[20,20,1]], dtype=float)
            return blobs

        vis_blobs = blob_detect(pic_rescale, min_sig, max_sig, thresh)
#---------------------------------------------------------------------------------------------#
        # SDM Background Filter: Removes blobs on high-contrast edges
        sdm_filter = 65 ###Make lower if edge particles are being detected
        if mirror_toggle is True: sdm_filter = sdm_filter / (np.mean(mirror) * 0.5)
        #This measures the percent contrast of blobs and removes blobs likely to not be particles
#---------------------------------------------------------------------------------------------#
        def particle_quant(image, d_blobs, particle_array, zslice, sdm_filter):
            i = 0
            perc_contrast, back_lum_sdm, zslice_list = [],[],[]
            for blob in d_blobs:
                y,x,r = d_blobs[i]
                y = int(y); x = int(x)
                r = int(math.ceil(r))
                point_lum = image[y,x]

                bg = image[y-(r):y+(r+1),x-(r):x+(r+1)]
                #bg = np.full([r+1,r+1], point_lum)

                try: bg_circ = np.hstack([bg[0,1:-1],bg[:,0],bg[-1,1:-1],bg[:,-1]])
                except IndexError:
                    bg = np.full([r+1,r+1], point_lum)
                    bg_circ = np.hstack([bg[0,1:-1],bg[:,0],bg[-1,1:-1],bg[:,-1]])
                    print(y,x, "there was an index error")

                bg_lum_avg = np.mean(bg_circ)
                bg_lum_sdm_pt = np.std(bg_circ) / math.sqrt(len(bg_circ))
                perc_contrast_pt = ((point_lum - bg_lum_avg) * 100) / bg_lum_avg
                perc_contrast.append([perc_contrast_pt])
                back_lum_sdm.append([bg_lum_sdm_pt])
                zslice_list.append([zslice])
                i += 1

            d_blobs = np.append(d_blobs, np.asarray(perc_contrast), axis = 1)
            d_blobs = np.append(d_blobs, np.asarray(back_lum_sdm), axis = 1)
            d_blobs = np.append(d_blobs, np.asarray(zslice_list), axis = 1)

            particles = d_blobs[(d_blobs[:,4] < sdm_filter) & (d_blobs[:,3] > 0)]
            if len(particles) == 0: particles = [[0,0,0,0,0,0]]
            print("\nImage scanned: " + str(pgmfile))
            particle_array = np.concatenate((particle_array, particles))
            print("Particles in image: " + str(len(particles)) + "\n")
            return particle_array
#---------------------------------------------------------------------------------------------#
        total_particles = particle_quant(pic, vis_blobs, total_particles, zslice, sdm_filter)

    particle_df = pd.DataFrame(total_particles)
    particle_df.rename(columns = {0:'y', 1:'x', 2:'r', 3:'pc', 4:'sdm', 5:'z'},inplace = True)
#---------------------------------------------------------------------------------------------#
    # Duplicate Particle Detector:  Removes duplicate particles by rounding method
#---------------------------------------------------------------------------------------------#
    def dupe_finder(DFrame):
        xrd5 = (DFrame.x/5).round()*5; yrd5 = (DFrame.y/5).round()*5
        xrd10 = DFrame.x.round(-1); yrd10 = DFrame.y.round(-1)
        xceil = np.ceil(DFrame.x/10)*10; yceil = np.ceil(DFrame.y/10)*10
        xfloor = np.floor(DFrame.x/10)*10; yfloor = np.floor(DFrame.y/10)*10
        DFrame['yx_5'] = pd.Series(list(zip(yrd5,xrd5)))
        DFrame['yx_10'] = pd.Series(list(zip(yrd10,xrd10)))
        DFrame['yx_5/10'] = pd.Series(list(zip(yrd5,xrd10)))
        DFrame['yx_10/5'] = pd.Series(list(zip(yrd10,xrd5)))
        DFrame['yx_ceil'] = pd.Series(list(zip(yceil,xceil)))
        DFrame['yx_floor'] = pd.Series(list(zip(yfloor,xfloor)))
        return DFrame

    rounding_cols = ['yx_5','yx_10','yx_10/5','yx_5/10','yx_ceil','yx_floor']
    def dupe_dropper(DFrame, cols_to_drop, sorting_col):
        DFrame.sort_values([sorting_col], kind = 'quicksort', inplace = True)
        for column in cols_to_drop:
            DFrame.drop_duplicates(subset = (column), keep = 'last', inplace = True)
        # DFrame.drop(['yx_5','yx_10','yx_10/5','yx_5/10','yx_ceil','yx_floor'],
        #                  axis = 1, inplace = True)
        DFrame.reset_index(drop = True, inplace = True)
        return DFrame

    particle_df = dupe_finder(particle_df)
    particle_df = dupe_dropper(particle_df, rounding_cols, sorting_col = 'pc')

    particle_count = len(particle_df)
    print("Unique particles counted: " + str(particle_count) +"\n")

#---------------------------------------------------------------------------------------------#
#### Fluorescent File Processer WORK IN PRORGRESS
    min_sig = 0.9; max_sig = 2; thresh = .12
#---------------------------------------------------------------------------------------------#
    if fluor_files:
        fluor_particles = np.empty(shape = (0,6))
        for file in fluor_files:
            pic_fluor = io.imread(file)
            if mirror_toggle == True:
                pic_fluor = pic_fluor / mirror
            p1,p2 = np.percentile(pic_fluor, (2, 98))
            if p2 < 0.05: p2 = 0.05
            #print(p1,p2)
            pic_fluor_rescale = exposure.rescale_intensity(pic_fluor, in_range=(p1,p2))
            fluor_to_count = pic_fluor_rescale.copy()

            fluor_to_count[outer_disk_mask] = fluor_to_count.max()
            pic_fluor[outer_disk_mask] = pic_fluor.max()

            fluor_to_count = border_masker(fluor_to_count)
            pic_fluor = border_masker(pic_fluor)

            fluor_blobs = blob_detect(fluor_to_count, min_sig, max_sig, thresh)
            #print(len(fluor_blobs))

            fluor_particles = particle_quant(pic_fluor, fluor_blobs,
                                             fluor_particles, zslice, sdm_filter)


            fluor_part_df = pd.DataFrame(fluor_particles)
            fluor_part_df.rename(columns = {0:'y', 1:'x', 2:'r', 3:'pc', 4:'sdm', 5:'z'},
                                    inplace = True)
            fluor_part_df.z.replace(to_replace = zslice, value = 'A', inplace = True)
            #print(fluor_part_df)

            figsize = (ncols/dpi, nrows/dpi)
            fig = plt.figure(figsize = figsize, dpi = dpi)
            axes = plt.Axes(fig,[0,0,1,1])
            fig.add_axes(axes)
            axes.set_axis_off()
            axes.imshow(pic_fluor_rescale, cmap = 'plasma')

            ab_spot = plt.Circle((cx, cy), rad, color='w',linewidth=5, fill=False, alpha = 0.5)
            axes.add_patch(ab_spot)

            yf = fluor_part_df.y
            xf = fluor_part_df.x
            pcf = fluor_part_df.pc
            for i in range(0,len(pcf)):
                point = plt.Circle((xf[i], yf[i]), pcf[i] * .0025,
                                    color = 'white', linewidth = 1,
                                    fill = False, alpha = 1)
                axes.add_patch(point)

            bin_no = 55
            ax_hist = plt.axes([.375, .05, .25, .25])
            pixels_f, hbins_f, patches_f = ax_hist.hist(pic_fluor_rescale.ravel(), bin_no, facecolor = 'red', normed = True)
            ax_hist.patch.set_alpha(0.5)
            ax_hist.patch.set_facecolor('black')
            plt.show()
            plt.clf()
            plt.close('all')


#---------------------------------------------------------------------------------------------#
####Processed Image Renderer
    pic_to_show = pic_rescale_mid
#---------------------------------------------------------------------------------------------#
    #title = png + " Particles"
    figsize = (ncols/dpi, nrows/dpi)
    fig = plt.figure(figsize = figsize, dpi = dpi)
    axes = plt.Axes(fig,[0,0,1,1])
    fig.add_axes(axes)
    axes.set_axis_off()
    colormap = ['#a50026','#d73027','#f46d43','#fdae61','#fee090','#ffffbf','#e0f3f8','#abd9e9','#74add1','#4575b4','#313695']

    axes.imshow(pic_to_show, cmap = 'gray')
    ab_spot = plt.Circle((cx, cy), rad, color='#5A81BB',
                  linewidth=5, fill=False, alpha = 0.5)
    axes.add_patch(ab_spot)


    z_list = [int(z) for z in list(set(particle_df.z))]
    pc_hist = list()
    ax_hist = plt.axes([.06, .7, .25, .25])
    hist_max = 6
    for zslice in z_list:
        y = particle_df.loc[particle_df.z == zslice].y.reset_index(drop = True)
        x = particle_df.loc[particle_df.z == zslice].x.reset_index(drop = True)
        pc = particle_df.loc[particle_df.z == zslice].pc.reset_index(drop = True)
        if max(pc) > hist_max: hist_max = max(pc)
        pc_hist.append(np.array(pc))
        for i in range(0,len(pc)):
            point = plt.Circle((x[i], y[i]), pc[i] * 2.5,
                                color = colormap[zslice-1], linewidth = 1,
                                fill = False, alpha = 1)
            axes.add_patch(point)
    #print(pc_hist[0])
    # yf = fluor_part_df.y
    # xf = fluor_part_df.x
    # pcf = fluor_part_df.pc
    # for i in range(0,len(pcf)):
    #     point = plt.Circle((xf[i], yf[i]), 2.5,#pcf[i] * .1,
    #                         color = 'white', linewidth = 1,
    #                         fill = True, alpha = 1)
    #     axes.add_patch(point)
    #if particle_count > len(z_list):
    print(len(pc_hist))
    hist_vals, hbins, hist_patches = ax_hist.hist(pc_hist, bins = 200, range = [0,30],
                                                  linewidth = 2, alpha = 0.5,stacked = True,
                                                  color = colormap[:len(pc_hist)],
                                                  label = z_list)
    ax_hist.patch.set_alpha(0.5)
    ax_hist.patch.set_facecolor('black')
    ax_hist.legend(loc = 'best')

    if math.ceil(np.median(pc)) > 6: hist_x_axis = math.ceil(np.median(pc)*2.5)
    else: hist_x_axis = 6
    ax_hist.set_xlim([0,np.ceil(hist_max)])
    for spine in ax_hist.spines: ax_hist.spines[spine].set_color('k')
    ax_hist.tick_params(color = 'k')
    plt.title("PARTICLE CONTRAST DISTRIBUTION", size = 12, color = 'k')
    plt.xticks(size = 10, color = 'k')
    plt.xlabel("% CONTRAST", size = 12, color = 'k')
    plt.yticks(size = 10, color = 'k')
    plt.ylabel("PARTICLE COUNT", color = 'k')

    if not os.path.exists('../virago_output/'+ chip_name + '/processed_images'):
        os.makedirs('../virago_output/' + chip_name + '/processed_images')
    plt.savefig('../virago_output/' + chip_name + '/processed_images/' + png +'.png', dpi = dpi)
    print("Processed image generated: " + png + ".png")
    plt.show()
    plt.close()


    # scatter_df = particle_df[(particle_df.pc <= 25) & (particle_df.pc > 10)]

    # fig = plt.figure(figsize = figsize, dpi = dpi)
    # subplot = fig.add_subplot(111)
    #
    # subplot.scatter(scatter_df.x,scatter_df.y,c='r', marker = 'o', alpha = 0.5)
    # subplot.scatter(fluor_part_df.x,fluor_part_df.y,c='g', marker = '+', alpha = 0.5)
    # plt.show()
    # plt.close()


    #vis_fluor_df = pd.concat([particle_df, fluor_part_df])
    #vis_fluor_df = dupe_finder(vis_fluor_df)
    #print(vis_fluor_df)
    if fluor_files:
        fluor_part_df = dupe_finder(fluor_part_df)
        #rounding_cols = ['yx_5','yx_10','yx_10/5','yx_5/10','yx_ceil','yx_floor']
        merge_df = pd.DataFrame()
        merging_cols_drop = ['yx_5_x','yx_10_x','yx_10/5_x','yx_5/10_x','yx_ceil_x','yx_floor_x',
                        'yx_5_y','yx_10_y','yx_10/5_y','yx_5/10_y','yx_ceil_y','yx_floor_y']
        merging_cols_keep = ['y_x', 'x_x', 'r_x', 'pc_x']
        for column in rounding_cols:
            merge_df2 = pd.merge(particle_df, fluor_part_df, how = 'inner', on = [column])
            #merge_df2.drop(merging_cols_drop, axis = 1, inplace = True)
            print(merge_df2)
            merge_df.append(merge_df2, ignore_index = True)
        print(merge_df)
        if not merge_df.empty:

            merge_df = dupe_dropper(merge_df, merging_cols, sorting_col = 'pc_x')
            merge_df.drop(rounding_cols, axis = 1, inplace = True)
            merge_df.drop(merging_cols, axis = 1, inplace = True)
            print(merge_df)
            print(len(merge_df))
        # merge_df.drop(['yx_5','yx_10/5','yx_5/10','yx_ceil','yx_floor'],
        #                     axis = 1, inplace = True)
        # merge_df.fillna(0, inplace=True)
        #
        # nonmatches = (merge_df.pc_y == 0).sum()
        # print(nonmatches / len())
            fig = plt.figure(figsize = figsize, dpi = dpi)
            subplot = fig.add_subplot(111)
            subplot.scatter(merge_df.pc_x,merge_df.pc_y, c='g', marker = '+', alpha = 0.5)
            subplot.set_xlabel("Visible Percent Contrast", color = 'gray')
            subplot.set_ylabel("Fluorescent Percent Contrast", color = 'gray')
            plt.title = (png + ": Correlation of Visible Particle Size with Fluorescent Signal")
            plt.show()
            plt.close()

    particle_df.drop(rounding_cols, axis = 1, inplace = True)
    if not os.path.exists('../virago_output/'+ chip_name + '/vcounts'):
        os.makedirs('../virago_output/' + chip_name + '/vcounts')
    particle_df.to_csv('../virago_output/' + chip_name + '/vcounts/' + png + '.'
                         + str(area_squm) + '.vcount.csv', sep = ",")
#---------------------------------------------------------------------------------------------#
    return particle_count, particle_df
#*********************************************************************************************#
def nano_csv_reader(chip_name, spot_data, csv_list):
    min_corr = input("\nWhat is the correlation cutoff for particle count?"+
                     " (choose value between 0.5 and 1)\t")
    if min_corr == "": min_corr = 0.75
    min_corr = float(min_corr)
    contrast_window = input("\nEnter the minimum and maximum percent contrast values," +
                            " separated by a comma (for VSV, 0-6% works well)\t")
    assert isinstance(contrast_window, str)
    contrast_window = contrast_window.split(",")
    cont_0 = (float(contrast_window[0])/100)+1
    cont_1 = (float(contrast_window[1])/100)+1
    min_corr_str = str("%.2F" % min_corr)
    particles_list = ([])
    particle_dict = {}
    nano_csv_list = [csvfile for csvfile in csv_list if csvfile.split(".")[-2].isdigit()]
    for csvfile in nano_csv_list: ##This pulls particle data from the CSVs generated by nanoViewer
        csv_data = pd.read_table(
                             csvfile, sep = ',',
                             error_bad_lines = False, usecols = [1,2,3,4,5],
                             names = ("contrast", "correlation", "x", "y", "slice")
                             )
        filtered = csv_data[(csv_data['contrast'] <= cont_1)
                    & (csv_data['contrast'] > cont_0)
                    & (csv_data['correlation'] >= min_corr)][['contrast','correlation']]
        particles = len(filtered)
        csv_id = csvfile.split(".")[1] + "." + csvfile.split(".")[2]
        particle_dict[csv_id] = list(round((filtered.contrast - 1) * 100, 4))
        particles_list.append(particles)
        print('File scanned: '+ csvfile + '; Particles counted: ' + str(particles))
        particle_count_col = str('particle_count_'+ min_corr_str
                           + '_' + contrast_window[0]
                           + '_' + contrast_window[1]+ '_')
    spot_data[particle_count_col] = particles_list
    for row in spot_data.iterrows():
        filtered_density = spot_data[particle_count_col] / spot_data.area / 1000
    spot_data = pd.concat([spot_data, filtered_density.rename('kparticle_density')], axis = 1)
    dict_file = pd.io.json.dumps(particle_dict)
    f = open('../virago_output/' + chip_name + '/' + chip_name + '_particle_dict_' + min_corr_str + 'corr.txt', 'w')
    f.write(dict_file)
    f.close()
    print("Particle dictionary file generated")

    return min_corr, spot_data, particle_dict, contrast_window
#*********************************************************************************************#
#*********************************************************************************************#
def virago_csv_reader(chip_name, csv_list, vir_toggle):
    if vir_toggle is False:
        min_corr = input("\nWhat is the correlation cutoff for particle count?"+
                         " (choose value between 0.5 and 1)\t")
        if min_corr == "": min_corr = 0.75
        min_corr = float(min_corr)
        min_corr_str = str("%.2F" % min_corr)
    contrast_window = str(input("\nEnter the minimum and maximum percent contrast values," +
                                "separated by a comma (for VSV, 0.5-6% works well)\t"))
    contrast_window = contrast_window.split(",")
    particles_list = ([])
    particle_dict = {}

    for csvfile in csv_list: ##This pulls particle data from the CSVs generated by VIRAGO
        csv_info = csvfile.split(".")
        csv_data = pd.read_table(
                                 csvfile, sep = ',', skiprows = [0],
                                 error_bad_lines = False, usecols = [1,2,3,4,5,6],
                                 header = None, names = ("y", "x", "r", "pc", "sdm",'z')
                                )
        #print(csv_data)
        kept_particles = [float(val) for val in csv_data.pc if float(contrast_window[0]) < float(val)
                                                  <= float(contrast_window[1])]

        particle_count = len(kept_particles)

        csv_id = str(csv_info[1])+"."+str(csv_info[2])
        particle_dict[csv_id] = kept_particles
        particles_list.append(particle_count)
        print('File scanned:  '+ csvfile + '; Particles counted: ' + str(particle_count))
    dict_file = pd.io.json.dumps(particle_dict)
    #os.chdir('../virago_output/' + chip_name + '/')
    f = open(chip_name + '_particle_dict_vir.txt', 'w')
    f.write(dict_file)
    f.close()
    print("Particle dictionary file generated")

    return particles_list, contrast_window, particle_dict
#*********************************************************************************************#
#*********************************************************************************************#
def joyplot(min_corr, chip_name, mAb_dict, contrast_window):

    min_corr_str = str("%.2F" % min_corr)
    min_cont = float(contrast_window[0])
    max_cont = float(contrast_window[1])

    with open('../virago_output/' + chip_name + '/'
        + chip_name + '_particle_dict_'
        + min_corr_str + 'corr.txt', 'r') as infile:
        particle_dict = json.load(infile)

    particle_df = pd.DataFrame.from_dict(particle_dict, orient = 'index').T
    particle_df = particle_df.sort_index(axis = 1, ascending = False)

    ##Joyplot generator
    spots_to_hist = str(input("Which spots would you like to generate joyplots for?"+
    "(Enter all spots you want separated by a comma)\t"))
    spots_to_list = spots_to_hist.split(",")

    i = 0
    joydf = pd.DataFrame([])
    for col in particle_df.columns:
        spot_scan = particle_df.columns[i]
        hist_spot = int(spot_scan[:3])
        if str(hist_spot) in spots_to_hist:
            current_spot = int(spots_to_hist.index(str(hist_spot)))
            joydf = pd.concat([joydf, particle_df.iloc[:,i]], axis = 1)
            i += 1
            continue
        else:
            i += 1
            continue

    fig, ax = joypy.joyplot(joydf, kind = 'counts', bins = 60,
                            ylim = 'max', overlap = 1.25, figsize = (6,5),
                            fade = True, color = '#377eb8', linewidth = 0.5,
                            grid = True, x_range = (min_cont,max_cont), ylabelsize = 3)
    plt.ylabel("Particle Frequency")
    plt.xlabel("% Contrast", color = 'k')
    plt.title(chip_name + ": Particle Frequency Distributions above "+min_corr_str+" Correlation\n"
                        + "All Passes of "
                        + mAb_dict[int(spots_to_hist[current_spot])]
                        + ' Spot ' + str(spots_to_hist[current_spot]))
    #plt.show()
    fig.savefig('../virago_output/' + chip_name + '/' + chip_name +'_'
                + mAb_dict[int(spots_to_hist[current_spot])]
                + '_' + str(spots_to_hist[current_spot]) + '_joyplot.png',
                bbox_inches = 'tight', pad_inches = 0.1, dpi = 300)
    print('File generated: ' + chip_name + '_' + mAb_dict[int(spots_to_hist[current_spot])]
                             + '_Spot' + str(spots_to_hist[current_spot]) + '_joyplot.png')
    plt.close()
#*********************************************************************************************#
## Particle count normalizer so pass 1 = 0 particle density
#*********************************************************************************************#
def density_normalizer(spot_data, pass_counter, spot_list):
    normalized_density = []
    for spot in spot_list:
        normspot = [val[0] for val in spot_data.spot_number.iteritems() if int(val[1]) == spot]
        norm_d = [
                  (spot_data.kparticle_density[normspot[scan]]
                   - spot_data.kparticle_density[normspot[0]])
                  for scan in np.arange(0,pass_counter,1)
                 ]
        normalized_density = normalized_density + norm_d
    return normalized_density
#*********************************************************************************************#
#*********************************************************************************************#
def chip_file_reader(xml_file): ##XML file reader, reads the chip file for IRIS experiment
    xml_raw = etree.iterparse(xml_file)
    chip_dict = {}
    chip_file = []
    for action, elem in xml_raw:
        if not elem.text:
            text = "None"
        else:
            text = elem.text
        #print(elem.tag + " => " + text)
        chip_dict[elem.tag] = text
        if elem.tag == "spot":
            chip_file.append(chip_dict)
            chip_dict = {}
    return chip_file
#*********************************************************************************************#
#
#    CODE BEGINS HERE
#
#*********************************************************************************************#
##Point to the correct directory
retval = os.getcwd()
print("\nCurrent working directory is:\n %s" % retval)
iris_path = input("\nPlease type in the path to the folder that contains the IRIS data:\n")
os.chdir(iris_path.strip('"'))

txt_list = sorted(glob.glob('*.txt'))
pgm_list = sorted(glob.glob('*.pgm'))
csv_list = sorted(glob.glob('*.csv'))
xml_list = sorted(glob.glob('*/*.xml'))
if not xml_list: xml_list = sorted(glob.glob('../*/*.xml'))
chip_name = pgm_list[0].split(".")[0]
mirror_file = str(glob.glob('*000.pgm')).strip("'[]'")
if mirror_file:
    pgm_list.remove(mirror_file)
    print("Mirror file present")
else: print("Mirror file absent") #mirror_toggle = False

zslice_count = max([int(pgmfile.split(".")[3]) for pgmfile in pgm_list])
txtcheck = [file.split(".") for file in txt_list]
iris_txt = [".".join(file) for file in txtcheck if (len(file) >= 3) and (file[2].isalpha())]
nv_txt = [".".join(file) for file in txtcheck if (len(file) > 3) and (file[2].isdigit())]

### Important Value
if nv_txt: pass_counter = max([int(file[2]) for file in txtcheck if (len(file) > 3)])
###

xml_file = [file for file in xml_list if chip_name in file]
chip_file = chip_file_reader(xml_file[0])
intro = chip_file[0]
#*********************************************************************************************#
# This takes antibody names and makes them more general for easier layperson understanding
#*********************************************************************************************#
jargon_dict = {
               '13F6': 'anti-EBOVmay', '127-8': 'anti-MARV',
               '6D8': 'anti-EBOVmak', '8.9F': 'anti-LASV',
               '8G5': 'anti-VSV', '4F3': 'anti-panEBOV',
               '13C6': 'anti-panEBOV'
               }
q = 0
mAb_dict = {} ##Matches spot antibody type to scan order (spot number)
for spot in chip_file:
    spot_info_dict = chip_file[q]
    mAb_name = spot_info_dict['spottype'].upper()
    for key in jargon_dict:
        if mAb_name.endswith(key) or mAb_name.startswith(key):
            mAb_name = jargon_dict[key]
    mAb_dict[q + 1] = mAb_name
    q += 1
print(mAb_dict)
#*********************************************************************************************#
spot_counter = len([key for key in mAb_dict])##Important

sample_name = input("\nPlease enter a sample descriptor (e.g. VSV-MARV@1E6 PFU/mL)\n")
#if not os.path.exists('../virago_output/'): os.mkdir('../virago_output/')
if not os.path.exists('../virago_output/'+ chip_name): os.makedirs('../virago_output/' + chip_name)
##Varibles
averaged_data = []
normalized_density = ([])
spot_labels, spot_set = [], []
# hist_dict = {}
#*********************************************************************************************#
# Text file Parser
#*********************************************************************************************#

spot_data_nv = pd.DataFrame([])
spot_list = [int(file[1]) for file in txtcheck if (len(file) > 2) and (file[2].isalpha())]
scanned_spots = set(np.arange(1,spot_counter+1,1))
missing_spots = scanned_spots.difference(spot_list)
miss_txt = 1
for txtfile in iris_txt:
    if miss_txt in missing_spots:
        print('Missing text file:  ' + str(miss_txt))
        miss_list = pd.Series(list(str(miss_txt))*pass_counter)
        blanks = pd.DataFrame(np.zeros((pass_counter,3)))
        blanks.insert(0,'spot_number', miss_list)
        miss_txt += 1

    txtdata = pd.read_table(txtfile, sep = ':', error_bad_lines = False,
                            header = None, index_col = 0, usecols = [0, 1])
    pass_labels = [row for row in txtdata.index if row.startswith('pass_time')]
    if not nv_txt: pass_counter = int(len(pass_labels)) ##If nanoViewer hasn't run on data

    spot_idxs = pd.Series(list(txtdata.loc['spot_index']) * pass_counter)
    pass_list = pd.Series(np.arange(1,pass_counter + 1))
    spot_types = pd.Series(list([mAb_dict[int(txtfile.split(".")[1])]]) * pass_counter)

    times_s = pd.Series(txtdata.loc[pass_labels].values.flatten().astype(np.float))
    times_min = round(times_s / 60,2)
    pass_diff = pass_counter - len(pass_labels)
    if pass_diff > 0:
        times_min = times_min.append(pd.Series(np.zeros(pass_diff)), ignore_index = True)
    print('File scanned:  ' + txtfile)
    miss_txt += 1
    spot_data_solo = pd.concat([spot_idxs.rename('spot_number').astype(int),
                                pass_list.rename('scan_number').astype(int),
                                times_min.rename('scan_time'),
                                spot_types.rename('spot_type')], axis = 1)
    spot_data_nv = spot_data_nv.append(spot_data_solo, ignore_index = True)

spot_data_vir = spot_data_nv.copy()

area_col = []
for txtfile in nv_txt:
    if int(txtfile.split(".")[1]) in missing_spots:
        print("Did not scan " + txtfile + "; data missing")
    else:
        txtdata = pd.read_table(txtfile, sep = ':', error_bad_lines = False,
                                header = None, index_col = 0, usecols = [0, 1])
        area = float(txtdata.loc['area'])
        area_col.append(area)
        print('File scanned:  ' + txtfile)
area_col = pd.Series(area_col, name = 'area')

spot_data_nv['area'] = area_col
spot_data_nv.scan_time.replace(0, np.nan, inplace = True)

spot_labels = [[val]*(pass_counter) for val in mAb_dict.values()]
spot_labels2 = pd.Series(np.asarray(spot_labels).flatten())
for val in mAb_dict.values():
    if val not in spot_set: spot_set.append(val)

#*********************************************************************************************#
# PGM Scanning
spot = 7 ##Change this.......... to only scan certain spots
#*********************************************************************************************#

if pgm_list:
    pgm_toggle = input("\nPGM files exist. Do you want scan them for particles? (y/[n])\n"
                         + "WARNING: This will take a long time!\t")
    if pgm_toggle.lower() in ('yes', 'y'):
        pgm_toggle = True
        image_detail_toggle = input("Do you want to render image processing details? y/[n]?\t")
        startTime = datetime.now()
        pgm_set = set([".".join(file.split(".")[:3]) for file in pgm_list])

        while spot <= spot_counter:
            pass_per_spot_list = sorted([file for file in pgm_set if int(file.split(".")[1]) == spot])
            passes_per_spot = len(pass_per_spot_list)

            scan_range = range(0,passes_per_spot,1)
            for x in scan_range:
                scan_list = [file for file in pgm_list if file.startswith(pass_per_spot_list[x])]
                particle_count, particle_df = IRISpgm_scanner(mirror_file, scan_list, image_detail_toggle)
            if passes_per_spot != pass_counter: print("Missing pgm files... ")
            spot += 1

        print(datetime.now() - startTime)

#*********************************************************************************************#
# CSV Reader
#*********************************************************************************************#
def csv_fixer(csv_list, txtcheck, vir_toggle):
    csvcheck = set([(".".join(file[:-1])+'.csv') for file in txtcheck if (len(file) > 2)
                    and (file[2].isdigit())])
    miss_csv = list(csvcheck.difference(csv_list))
    blank_csv = np.array([[1,0,0,0,0,1]])
    for csv in miss_csv:
        if vir_toggle is True:
            vcount_str = '.0.vcount'
            csv = csv.split(".")
            csv = ".".join(csv[:3]) + vcount_str + '.csv'
        print("Missing particle data... Generating blank: ", csv)
        np.savetxt(csv, blank_csv, delimiter=",", fmt=('%i','%i','%i','%i','%i','%i'))
    csv_list = sorted(glob.glob('*.csv'))
    return csv_list

csv_list = csv_fixer(csv_list,txtcheck, vir_toggle = False)
#
# olddata_toggle = 'no'
# if os.path.exists('../virago_output/' + chip_name + '/' + chip_name + '_spot_data_nv.csv'):
#     olddata_toggle = input("\nPre-existing data exists. Do you want use this data? (y/n)\n"
#         "NOTE: Selecting 'no' will overwrite old data)\t")
#     assert isinstance(olddata_toggle, str)
#     if olddata_toggle.lower() in ('yes', 'y'):
#         ##This reads in the pre-existing data files and stores the needed variables
#         print("\nUsing pre-existing data...\n")
#         preexist_csv = '../virago_output/' + chip_name + '/' + chip_name + '_spot_data_nv.csv'
#         spot_data_nv = pd.read_table(preexist_csv, sep = ',', error_bad_lines = False)
#         count_info = [col for col in spot_data_nv.columns if col.startswith('particle_count')]
#         count_info = str(count_info).split("_")
#         min_corr = float(count_info[2])
#         contrast_window = count_info[3:5]
#         print(str(contrast_window[0])+"-"+contrast_window[1])
#         min_corr_str = str("%.2F" % min_corr)
#         print("Correlation value is: "+ min_corr_str +" or greater\n")
#         with open('../virago_output/' + chip_name + '/'
#             + chip_name + '_particle_dict_'
#             + min_corr_str + 'corr.txt', 'r') as infile:
#             particle_dict = json.load(infile)
#     elif olddata_toggle.lower() in ('no', 'n'):
#         print("")
#         min_corr, spot_data_nv, particle_dict, contrast_window = nano_csv_reader(chip_name, spot_data_nv, csv_list)
# else:
#     min_corr, spot_data_nv, particle_dict, contrast_window = nano_csv_reader(chip_name, spot_data_nv, csv_list)
# min_corr_str = str("%.2F" % min_corr)
os.chdir('../virago_output/'+ chip_name + '/vcounts')
vir_csv_list = sorted(glob.glob(chip_name +'*.vcount.csv'))
if pgm_toggle is True:
    vir_csv_list = [".".join(csv.split(".")[:3])+'.csv' for csv in vir_csv_list]
    vir_csv_list = csv_fixer(vir_csv_list,txtcheck, vir_toggle = True)
#os.chdir(iris_path.strip('"'))

if len(vir_csv_list) == (len(txt_list) - spot_counter):
    particle_count_vir, contrast_window, particle_dict = virago_csv_reader(chip_name, vir_csv_list, vir_toggle = True)

    area_list = np.array([(float(csvfile.split(".")[-3]) / 1e6) for csvfile in vir_csv_list])
    spot_data_vir['area_sqmm'] = area_list

    particle_count_col = str('particle_count_0'
                             + '_' + contrast_window[0]
                             + '_' + contrast_window[1] + '_')
    spot_data_vir[particle_count_col] = particle_count_vir

    kparticle_density = np.round(np.array(particle_count_vir) / area_list * 0.001,3)
    spot_data_vir['kparticle_density'] = kparticle_density
# if (olddata_toggle.lower() not in ('yes', 'y')):
    nv_vir_toggle = input("Would you like to use nanoViewer or VIRAGO data? (type N or V)\n")
    assert isinstance(nv_vir_toggle, str)
    if nv_vir_toggle.lower() in ('n','nanoViewer'):
        print("Using nanoViewer data...")
        spot_data = spot_data_nv
    else:
        print("Using VIRAGO data...")
        spot_data = spot_data_vir
        min_corr_str = ""
else:
    spot_data = spot_data_nv
os.chdir(iris_path.strip('"'))
# -------------------------------------------------------------------
#####################################################################
# Joyplot generator
#####################################################################
#--------------------------------------------------------------------
# if int(sys.version[0]) == 3:
#     joyplot(min_corr, chip_name, mAb_dict, contrast_window)
# -------------------------------------------------------------------
#####################################################################
# Histogram generator
#####################################################################
#--------------------------------------------------------------------
spots_to_hist = input("Which spots would you like to generate histograms for?\t")
hist_norm = False
hist_norm_toggle = input("Do you want to normalize the counts to a percentage? (y/[n])")
if hist_norm_toggle.lower() in ('y','yes'): hist_norm = True
spots_to_hist = spots_to_hist.split(",")
print(spots_to_hist)
#cont_0 = float(contrast_window[0])
cont_1 = float(contrast_window[1])
for spot in spots_to_hist:
    hist_dict = {}
    for key in sorted(particle_dict.keys()):
        hist_spot = int(key.split(".")[0])
        if hist_spot == int(spot): hist_dict[key] = particle_dict[key]
    nrows = 2
    ncols = math.ceil(pass_counter / 2)
    fig = plt.figure()
    plt.axis('off')

    if hist_norm_toggle == False:
        fig.text(0.06,0.6,"Particle Counts " + min_corr_str, fontsize = 10, rotation = 'vertical')
    elif hist_norm_toggle == True:
        fig.text(0.06,0.6,"Particle Frequency" + min_corr_str, fontsize = 10, rotation = 'vertical')
    fig.text(.4,0.04,"Percent Contrast", fontsize = 10)

    for key in sorted(hist_dict.keys()):
        hist_pass = int(key.split(".")[1])
        sbplt = fig.add_subplot(nrows,ncols,hist_pass)
        (hist_vals, bins, patches) = sbplt.hist([hist_dict[key]],
                                     100, range = [0,cont_1], color = ['#0088FF'],
                                     rwidth = 1, alpha = 0.75, normed = hist_norm)
        plt.xticks(np.arange(0, cont_1+1,2), size = 5)
        if max(hist_vals) <= 50: grads = 5
        elif max(hist_vals) <= 50: grads = 10
        else: grads = 25
        if hist_norm_toggle == False: plt.yticks(np.arange(0, (max(hist_vals)) + 10 , grads), size = 5)

        plt.title("Pass "+ str(hist_pass), size = 5)
        plt.grid(True, alpha = 0.5)

        if hist_norm_toggle == False: sbplt.set_ylim(0, (max(hist_vals)) + 10)
    plot_title = str("Particle Contrast Distribution of " + chip_name + " "
                    + spot_labels[int(spot)-1][0]
                    + " Spot " + spot)
    plt.suptitle(plot_title, size = 12)
    plt.subplots_adjust(wspace = 0.25)

    plt.savefig('../virago_output/' + chip_name + '/' + chip_name + '_spot-' + spot
                +  '_histo.png', bbox_inches = 'tight', dpi = 300)
    print('File generated: ' +  chip_name + '_spot-' + spot + '_histo.png')
    plt.close()


#*********************************************************************************************#
# Particle count normalizer so pass 1 = 0 particle density
#*********************************************************************************************#

normalized_density = density_normalizer(spot_data, pass_counter, spot_list)
len_diff = len(spot_data) - len(normalized_density)
if len_diff != 0:
    normalized_density = np.append(np.asarray(normalized_density),np.full(len_diff, np.nan))
spot_data['normalized_density'] = normalized_density
print(spot_data)
#*********************************************************************************************#
def spot_remover(spot_data):
    excise_toggle = input("Would you like to remove any spots from the dataset? (y/(n))\t")
    assert isinstance(excise_toggle, str)
    if exise_toggle.lower() in ('y','yes'):
        spots_to_excise = input("Which spots? (Separate all spot numbers by a comma)\t")
        spots_to_excise = spots_to_excise.split(",")


##IN PROGRESS








# -------------------------------------------------------------------
#####################################################################
# This gets the average values and standard deviations for each spot type
#####################################################################
#--------------------------------------------------------------------
k = 0
scan_series = spot_data.scan_number
for val in spot_set:
    x = 1
    for val in pass_labels:
        data_slice = spot_data[['spot_type', 'scan_time', 'kparticle_density',
                                'normalized_density']][(scan_series == x)
                                & (spot_data['spot_type'] == spot_set[k])]
        scan_time_mean = round(data_slice['scan_time'].mean(),2)
        filt_density_mean = round(data_slice['kparticle_density'].mean(),2)
        filt_density_std = round(np.std(data_slice['kparticle_density']),2)
        norm_density_mean = round(data_slice['normalized_density'].mean(),2)
        norm_density_std = round(np.std(data_slice['normalized_density']),4)
        avg_data = (spot_set[k],
                    spot_data.loc[x - 1,'scan_number'],
                    scan_time_mean,
                    filt_density_mean,
                    filt_density_std,
                    norm_density_mean,
                    norm_density_std)
        averaged_data.append(avg_data)
        x += 1
    k += 1
averaged_data = pd.DataFrame(averaged_data,
                             columns =  ['spot_type', 'scan_number', 'avg_scan_time',
                                         'avg_kparticle_density', 'kparticle_density_std',
                                         'avg_normalized_density', 'normalized_density_std']
                             )
# -------------------------------------------------------------------
#####################################################################
# Asks whether the time series should be set such that Time 0 == 0 particle density
#####################################################################
#--------------------------------------------------------------------
baseline_toggle = input("Do you want the time series chart normalized to baseline? ([y]/n)\t")
assert isinstance(baseline_toggle, str)
if baseline_toggle.lower() in ('no', 'n'):
    filt_toggle = 'kparticle_density'
    avg_filt_toggle = 'avg_kparticle_density'
    stdev_filt_toggle = 'kparticle_density_std'
else:
    filt_toggle = 'normalized_density'
    avg_filt_toggle = 'avg_normalized_density'
    stdev_filt_toggle = 'normalized_density_std'
    print("Normalizing...")

# -------------------------------------------------------------------
#####################################################################
# Time Series Generator
#####################################################################
#--------------------------------------------------------------------
colormap = ('#e41a1c','#377eb8','#4daf4a',
            '#984ea3','#ff7f00','#ffff33',
            '#a65628','#f781bf','gray','black')
fig = plt.figure(figsize = (8,6))
ax1 = fig.add_subplot(111)
n,c = 1,0
for key in mAb_dict.keys():
    time_x = spot_data[spot_data['spot_number'] == key]['scan_time'].reset_index(drop = True)
    density_y = spot_data[spot_data['spot_number'] == key][filt_toggle].reset_index(drop = True)
    while n > 1:
        if mAb_dict[n-1] != mAb_dict[n]:
            c += 1
            break
        else:
            break
    ax1.plot(time_x, density_y, marker = '+', linewidth = 1,
                 color = colormap[c], alpha = 0.4, label = '_nolegend_')
    n += 1
ax2 = fig.add_subplot(111)
n = 0
for spot in spot_set:
    avg_data = averaged_data[averaged_data['spot_type'].str.contains(spot)]
    avg_time_x = avg_data['avg_scan_time']
    avg_density_y = avg_data[avg_filt_toggle]
    errorbar_y = avg_data[stdev_filt_toggle]
    ax2.errorbar(avg_time_x, avg_density_y,
                    yerr = errorbar_y, marker = 'o', label = spot_set[n],
                    linewidth = 2, elinewidth = 1, capsize = 3,
                    color = colormap[n], alpha = 0.9, aa = True)

    n += 1
#min_corr_str = str(0)
ax2.legend(loc = 'upper left', fontsize = 8, ncol = 1)
plt.xlabel("Time (min)", color = 'gray')
plt.ylabel('Particle Density (kparticles/sq. mm)\n'+ contrast_window[0]+'-'+contrast_window[1]
            + '% Contrast, Correlation Value >=' + min_corr_str, color = 'gray')
plt.xticks(np.arange(0, max(spot_data.scan_time) + 1, 5), color = 'gray')
plt.yticks(color = 'gray')
plt.title(chip_name + ' Time Series of ' + sample_name)

plt.axhline(linestyle = '--', color = 'gray')
plot_name = chip_name + '_timeseries_' + min_corr_str + 'corr.png'

plt.savefig('../virago_output/' + chip_name + '/' +  plot_name,
            bbox_inches = 'tight', pad_inches = 0.1, dpi = 300)
print('File generated: ' + plot_name)
csv_spot_data = str('../virago_output/' + chip_name + '/' + chip_name + '_spot_data.csv')
spot_data.to_csv(csv_spot_data, sep = ',')
#plt.show()
plt.clf(); plt.close('all')
print('File generated: '+ csv_spot_data)
# -------------------------------------------------------------------
#####################################################################
# Bar Plot Generator
#####################################################################
#--------------------------------------------------------------------
first_scan = min(scan_series)
last_scan = max(scan_series)
baseline = (spot_data[scan_series == first_scan][['spot_type', 'kparticle_density']]).reset_index(drop = True)
post_scan = pd.Series(spot_data[scan_series == last_scan]['kparticle_density'],
                      name = 'post_scan').reset_index(drop = True)
difference = pd.Series(spot_data[scan_series == last_scan]['normalized_density'],
                    name = 'difference').reset_index(drop = True)
barplot_data = pd.concat([baseline, post_scan, difference], axis = 1)
#barplot_data.kparticle_density = barplot_data.kparticle_density * -1
baseline_avg, post_scan_avg, baseline_std, post_scan_std, diff_avg, diff_std = [],[],[],[],[],[]
for spot in spot_set:
    avg_data = barplot_data[barplot_data['spot_type'].str.contains(spot)]
    baseline_avg.append(np.mean(avg_data.kparticle_density))
    baseline_std.append(np.std(avg_data.kparticle_density))

    post_scan_avg.append(np.mean(avg_data.post_scan))
    post_scan_std.append(np.std(avg_data.post_scan))

    diff_avg.append(np.mean(avg_data.difference))
    diff_std.append(np.std(avg_data.difference))
fig,axes = plt.subplots(nrows = 1, ncols = 1, figsize = (5,4), sharey = True)
fig.subplots_adjust(left=0.08, right=0.98, wspace=0)
plt.suptitle("Experiment "+ chip_name + "- Final Scan difference versus Inital Scan\n"
             + "Sample Conditions: " + sample_name, size = 12)
#
# bar1 = axes[0].bar(np.arange(len(spot_set)), baseline_avg, width = 0.45, color = colormap[0],
#                    tick_label = spot_set, yerr = baseline_std,capsize = 4, alpha = 0.75)
#
# bar2 = axes[0].bar(np.arange(len(spot_set)) + 0.45, post_scan_avg, width = 0.45,
#                    color = colormap[1], tick_label = spot_set, yerr = post_scan_std,
#                    capsize = 4, alpha = 0.75)

axes.set_ylabel('Particle Density (kparticles/sq. mm)\n' + contrast_window[0]
            +'-'+contrast_window[1] + '% Contrast, Correlation Value >='
            + min_corr_str, color = 'k', size = 8)
bar3 = axes.bar(np.arange(len(spot_set)) + (0.45/2), diff_avg, width = 0.5,
                   color = colormap[3],tick_label = spot_set, yerr = diff_std, capsize = 4)
#plt.xticks(np.arange(len(spot_set)) + (0.45/2), spot_set, rotation = 45, size = 6)
# for ax in axes:
axes.yaxis.grid(True)
axes.set_xticklabels(spot_set, rotation = 45, size = 6)
axes.set_xlabel("Antibody", color = 'k', size = 8)

#plt.figlegend((bar3),'Final Scan Difference',loc = 'upper right', fontsize = 10)
barplot_name = (chip_name + '_' + contrast_window[0] + '-' + contrast_window[1]
                + '_'+  min_corr_str + '_barplot.png')
plt.savefig('../virago_output/' + chip_name + '/' + barplot_name,
            bbox_inches = 'tight', pad_inches = 0.1, dpi = 300)
print('File generated: '+ barplot_name)
#plt.show()
plt.clf(); plt.close('all')
