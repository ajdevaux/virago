#! /usr/local/bin/python3
from datetime import datetime
from future.builtins import input

from lxml import etree
#from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
#import matplotlib.image as mpimg
import matplotlib.mlab as mlab
import pandas as pd
import numpy as np
from scipy import stats
from skimage import exposure, feature, io, transform, filters, morphology
import glob, os, json, sys, math, warnings
if int(sys.version[0]) == 3:
    import joypy
#from funks import IRIScsv_reader, IRISpgm_scanner, joyplot, density_normalizer

pd.set_option('display.width', 1000)


#*********************************************************************************************#
#
#           FUNCTIONS
#
#*********************************************************************************************#
#*********************************************************************************************#
# This function will scan PGMs (portable graymaps) generated by the IRIS instrument. It
# dynamically locates the antibody spot and will count blobs (either LoG or DoG algorithm)
# inside the perimeter of the spot.
# It then filters out blobs based on SDM of the background and by assigning them to bins to
# prevent counting the same particle across several images.
#*********************************************************************************************#
def IRISpgm_scanner(spot, scan, scan_list):
    total_particles = np.empty(shape = (0,6))
    pic_dict = {}
    for file in scan_list:
        perc_contrast, back_lum_sdm, zslice_list = [],[],[]
        pgmfile = file
        pgm_name = pgmfile.split(".")
        chip_name = str(pgm_name[0])
        zslice = int(pgm_name[3])
        png = '.'.join(pgm_name[:3])
        pic = io.imread(pgmfile)
        # pic_dict[png] = pic
#---------------------------------------------------------------------------------------------#
# ANTIBODY SPOT DETECTION:
# Decrease canny_sig if spots are not being detected accurately; increase if spot detection
# takes too long.
        canny_sig = 5.5; clip_lim_hc = 0.03
#---------------------------------------------------------------------------------------------#

        #pic_histo = exposure.histogramp(pic)
        warnings.simplefilter("ignore", UserWarning)

            #canny_sig = 7
        pic_eq = exposure.equalize_adapthist(pic, clip_limit = clip_lim_hc, nbins = 512)
        p1, p99 = np.percentile(pic, (1, 99))
        # pic_eq = exposure.rescale_intensity(pic, in_range=(p1, p99))
        if p1 < 28000:
            p1 = 28000
        print(p1, p99)

        nrows, ncols = pic.shape
        row, col = np.ogrid[:nrows,:ncols]
        if zslice == 4:
            pic_eq_mid = exposure.equalize_adapthist(pic, clip_limit = clip_lim_hc, nbins = 512)
            # p1, p99 = np.percentile(pic_eq, (1, 99))
            # print(p1,p99)
            # pic_hc_mid = exposure.rescale_intensity(pic_eq, in_range=(p1, p99))
        if zslice == 1:
            print("Locating antibody spot...")
            pic_eq_1 = exposure.equalize_adapthist(pic, clip_limit = 0.4)
            pic_selem = filters.rank.equalize(pic_eq_1, selem = morphology.disk(50))
            pic_edge = feature.canny(pic_selem, sigma = canny_sig)
            hough_radius = np.arange(500, 600, 25)
            hough_res = transform.hough_circle(pic_edge, hough_radius)
            accums, cx, cy, rad = transform.hough_circle_peaks(hough_res, hough_radius,
                                                                 total_num_peaks=1)
            if cx < ncols * 0.25 or cx > ncols * 0.75:
                cx = ncols * 0.5
                cy = nrows * 0.5
                rad = 270
            print(cx,cy,rad)
            a = (row - cy)
            b = (col - cx)
            c = (rad - 20)
            outer_disk_mask = (a**2 + b**2 > c**2)
        pic_eq[outer_disk_mask] = pic_eq.max()
        pic[outer_disk_mask] = pic.max()
        border_mask = 5
        pic_eq[0:border_mask,:], pic_eq[-(border_mask):,:] = pic_eq.max(), pic_eq.max()
        pic_eq[:,0:border_mask], pic_eq[:,-(border_mask):] = pic_eq.max(), pic_eq.max()
        pic[0:border_mask,:], pic[-(border_mask):,:] = pic.max(), pic.max()
        pic[:,0:border_mask], pic[:,-border_mask:] = pic.max(), pic.max()
        pix_area = (pic != pic.max()).sum()
        pix_sz_micron = 5.86
        mag = 40
        area_sqmm = ((pix_area * (pix_sz_micron)**2) / mag**2)*1e-6

#---------------------------------------------------------------------------------------------#
# BLOB DETECTION: Make threshold lower to detect more particles; make min_sigma lower to
#                 detect smaller particles
        min_sig = 1.5; max_sig = 3; thresh = .06
#---------------------------------------------------------------------------------------------#
        blobs = feature.blob_dog(
                                 pic_eq, min_sigma = min_sig, max_sigma = max_sig,
                                 threshold = thresh, overlap = 0
                                 )
        #print(blobs)
        blobs[:,2] = blobs[:,2] * math.sqrt(2)
        # blobs[:,0] = (nrows + border_mask) < blobs[:,0] < (nrows - border_mask)
        # blobs[:,1] = (ncols + border_mask) < blobs[:,1] < (ncols - border_mask)
        if len(blobs) == 0:
            blobs = np.asarray([[0,0,1],[20,20,1]], dtype=float)
#---------------------------------------------------------------------------------------------#
# SDM Background Filter: Removes blobs on high-contrast edges
        sdm_filter = 65 ###Make lower if edge particles are being detected
#---------------------------------------------------------------------------------------------#
        i = 0
        for blob in blobs:
            y,x,r = blobs[i]
            y = int(y)
            x = int(x)
            r = int(math.ceil(r))
            point_lum = pic[y,x]
            # if y > (nrows - r):
            #     bg = pic[y-(r):y+1,x-(r):x+(r+1)]
            #     bg_circ = np.hstack([bg[0,1:-1],bg[:,0],bg[:,-1]])
            # else:
            bg = pic[y-(r):y+(r+1),x-(r):x+(r+1)]
            bg_circ = np.hstack([bg[0,1:-1],bg[:,0],bg[-1,1:-1],bg[:,-1]])
            bg_lum_avg = np.mean(bg_circ)
            bg_lum_sdm_pt = np.std(bg_circ) / math.sqrt(len(bg_circ))
            perc_contrast_pt = ((point_lum - bg_lum_avg) * 100) / bg_lum_avg
            perc_contrast.append([perc_contrast_pt])
            back_lum_sdm.append([bg_lum_sdm_pt])
            zslice_list.append([zslice])
            i += 1
        blobs = np.append(blobs, np.asarray(perc_contrast), axis = 1)
        blobs = np.append(blobs, np.asarray(back_lum_sdm), axis = 1)
        blobs = np.append(blobs, np.asarray(zslice_list), axis = 1)
        particles = blobs[(blobs[:,4] < sdm_filter) & (blobs[:,3] > 0)]
        if len(particles) == 0:
            particles = [[0,0,0,0,0,0]]
        print("\nImage scanned: " + str(pgmfile))
        total_particles = np.concatenate((total_particles, particles))
    # dict_file = pd.io.json.dumps(pic_dict)
    # f = open(particles_list/' + chip_name + '/' + png + '_image_dict.txt', 'w')
    # f.write(dict_file)
    # f.close()
    # print("Image dictionary file generated")
#---------------------------------------------------------------------------------------------#
# Duplicate Particle Binner:  Combines duplicate particles by binning method
    bins = 100 ##Lower bin count if there are too many overlapping particles
#---------------------------------------------------------------------------------------------#
    total_part_df = pd.DataFrame(total_particles)
    total_part_df.rename(columns = {0:'y', 1:'x', 2:'r', 3:'pc', 4:'sdm', 5:'z'},
                         inplace = True)

    y = total_part_df['y']
    x = total_part_df['x']
    pc = total_part_df['pc']
    yx = np.asarray(total_part_df.loc[:,'y':'x'])
    twoD_hist_particles, xedges, yedges, binno = stats.binned_statistic_2d(x,y,pc,
                                                                           statistic = 'max',
                                                                           bins = bins)
    xbins = [(a + b) / 2 for a, b in zip(xedges[::1], xedges[1::])]
    ybins = [(a + b) / 2 for a, b in zip(yedges[::1], yedges[1::])]

    twoD_hist_particles = pd.DataFrame(twoD_hist_particles)
    twoD_hist_particles = twoD_hist_particles.T ##It needs to be transposed for some reason
    twoD_hist_particles.columns = [xbins]
    twoD_hist_particles.index = [ybins]

    uniq_particles = twoD_hist_particles.stack()
    uniq_particles = uniq_particles.reset_index(level = [0,1])
    uniq_particles.rename(columns = {'level_0':'y', 'level_1':'x', 0:'pc'}, inplace = True)
    round(uniq_particles.y,0); round(uniq_particles.x,0)
    uniq_particles.to_csv('../virago_output/' + chip_name + '/' + png +".uniq_particles.csv", sep = ",")

    particle_count = len(uniq_particles)
    print("Particles counted: " + str(particle_count))
#---------------------------------------------------------------------------------------------#
    pic_to_show = pic_eq_mid
#---------------------------------------------------------------------------------------------#
    title = png + " Particles"
    fig, axes = plt.subplots(1,1)
    axes.set_title(title)
    axes.imshow(pic_to_show, interpolation = None, cmap = 'gray')
    axes.axis('off')
    y = uniq_particles['y']
    x = uniq_particles['x']
    pc = uniq_particles['pc']
    #print(uniq_particles)
    i = 0

    for index in uniq_particles.iterrows():
        point = plt.Circle((x[i], y[i]), pc[i] * 2,
                            color='#FFB2AD', linewidth=0.5,
                            fill=False, alpha = 0.65)
        ab_spot = plt.Circle((cx, cy), (rad-10), color='#5A81BB',
                          linewidth=(10/3), fill=False, alpha = 0.15)
        axes.add_patch(point)
        axes.add_patch(ab_spot)
        axes.set_axis_off
        i += 1
    plt.savefig('../virago_output/' + chip_name + '/' + png +'.png', dpi = 450)
    #plt.show()
    plt.close()

    return particle_count, uniq_particles, area_sqmm
#*********************************************************************************************#
def nano_csv_reader(chip_name, spot_data):
    min_corr = float(input("\nWhat is the correlation cutoff for particle count? (choose value between 0.5 and 1)\t"))
    contrast_window = str(input("\nEnter the minimum and maximum percent contrast values, separated by a comma (for VSV, 0-6% works well)\t"))
    contrast_window = contrast_window.split(",")
    cont_0 = (float(contrast_window[0])/100)+1
    cont_1 = (float(contrast_window[1])/100)+1
    min_corr_str = str("%.2F" % min_corr)
    particles_list = ([])
    particle_dict = {}

    csv_list = sorted(glob.glob('*.csv'))
    for file in csv_list: ##This pulls particle data from the CSVs generated by nanoViewer
        csvfile = file
        csv_info = csvfile.split(".")
        if len(csv_info) >= 3 and csv_info[2].isdigit():
            csv_data = pd.read_table(
                                 csvfile, sep = ',',
                                 error_bad_lines = False, usecols = [1,2,3,4,5],
                                 names = ("contrast", "correlation", "x", "y", "slice")
                                 )
            filtered = csv_data[(csv_data['contrast'] <= cont_1)
                        & (csv_data['contrast'] > cont_0)
                        & (csv_data['correlation'] >= min_corr)][['contrast','correlation']]
            particles = len(filtered)
            csv_id = str(csv_info[1])+"."+str(csv_info[2])
            particle_dict[csv_id] = list(round((filtered.contrast - 1) * 100, 4))
            particles_list.append(particles)
            print('File scanned: '+ csvfile + '; Particles counted: ' + str(particles))
        else:
            print('Unrecognized file: ' + csvfile)
    particle_count_col = str('particle_count_'+ min_corr_str
                       + '_' + contrast_window[0]
                       + '_' + contrast_window[1]+ '_')
    spot_data[particle_count_col] = particles_list
    for row in spot_data.iterrows():
        filtered_density = (spot_data.loc[:,particle_count_col]
                          / spot_data.loc[:,'area']) / 1000
    spot_data = pd.concat([spot_data, filtered_density.rename('kparticle_density')], axis = 1)
    dict_file = pd.io.json.dumps(particle_dict)
    f = open('../virago_output/' + chip_name + '/' + chip_name + '_particle_dict_' + min_corr_str + 'corr.txt', 'w')
    f.write(dict_file)
    f.close()
    print("Particle dictionary file generated")

    return min_corr, spot_data, particle_dict, contrast_window
#*********************************************************************************************#
#*********************************************************************************************#
def virago_csv_reader(chip_name, spot_data):
    contrast_window = str(input("\nEnter the minimum and maximum percent contrast values, separated by a comma (for VSV, 0-6% works well)\t"))
    contrast_window = contrast_window.split(",")
    particles_list = ([])
    particle_dict = {}

    csv_list = sorted(glob.glob('../virago_output/' + chip_name + '/' + '*uniq_particles.csv'))
    for file in csv_list: ##This pulls particle data from the CSVs generated by nanoViewer
        csvfile = file
        csv_info = csvfile.split(".")
        csv_data = pd.read_table(csvfile, sep = ',', error_bad_lines = False,
                                 usecols = [1,2,3], header = 0, dtype = 'float')

        contrast = [val for val in csv_data.pc if float(contrast_window[0]) < val
                                                  <= float(contrast_window[1])]
        # for row in csv_data.iterrows():
        #     contrast = csv_data.get_value(k, "PC")
        particles = len(contrast)
        # if float(contrast_window[0]) < contrast <= float(contrast_window[1]):
        #     filter_df = pd.DataFrame([contrast]).T
        #     particles += 1
        print(particles)

        csv_id = str(csv_info[3])+"."+str(csv_info[4])
        particle_dict[csv_id] = contrast
        particles_list.append(particles)
        print('File scanned:  '+ csvfile + '; Particles counted: ' + str(particles))
    dict_file = pd.io.json.dumps(particle_dict)
    f = open('../virago_output/' + chip_name + '/' + chip_name + '_particle_dict_vir.txt', 'w')
    f.write(dict_file)
    f.close()
    print("Particle dictionary file generated")


    spot_data_csv = glob.glob('../virago_output/' + chip_name + '/' +'*vir.csv')
    spot_data = pd.read_csv(spot_data_csv[0], usecols = [1,2,3,4,5])
    print(spot_data)

    particle_count_col = str('particle_count_0'
                             + '_' + contrast_window[0]
                             + '_' + contrast_window[1]+ '_')
    spot_data[particle_count_col] = particles_list


    for row in spot_data.iterrows():
        filtered_density = (spot_data.loc[:,particle_count_col]
                          / spot_data.loc[:,'area_sqmm']) * 0.001

    spot_data['kparticle_density'] = filtered_density

    return spot_data, contrast_window, particle_dict
#*********************************************************************************************#
#*********************************************************************************************#
def joyplot(min_corr, chip_name, spot_dict, contrast_window):

    min_corr_str = str("%.2F" % min_corr)
    min_cont = float(contrast_window[0])
    max_cont = float(contrast_window[1])

    with open('../virago_output/' + chip_name + '/'
        + chip_name + '_particle_dict_'
        + min_corr_str + 'corr.txt', 'r') as infile:
        particle_dict = json.load(infile)

    particle_df = pd.DataFrame.from_dict(particle_dict, orient = 'index').T
    particle_df = particle_df.sort_index(axis = 1, ascending = False)

    ##Joyplot generator
    spots_to_hist = str(input("Which spots would you like to generate joyplots for? (Enter all spots you want separated by a comma)\t"))
    spots_to_list = spots_to_hist.split(",")

    i = 0
    joydf = pd.DataFrame([])
    for col in particle_df.columns:
        spot_scan = particle_df.columns[i]
        hist_spot = int(spot_scan[:3])
        if str(hist_spot) in spots_to_hist:
            current_spot = int(spots_to_hist.index(str(hist_spot)))
            joydf = pd.concat([joydf, particle_df.iloc[:,i]], axis = 1)
            i += 1
            continue
        else:
            i += 1
            continue

    fig, ax = joypy.joyplot(joydf, kind = 'counts', bins = 60,
                            ylim = 'max', overlap = 1.25, figsize = (6,5),
                            fade = True, color = '#377eb8', linewidth = 0.5,
                            grid = True, x_range = (min_cont,max_cont), ylabelsize = 3)
    plt.ylabel("Particle Frequency")
    plt.xlabel("% Contrast", color = 'k')
    plt.title(chip_name + ": Particle Frequency Distributions above "+min_corr_str+" Correlation\n"
                        + "All Passes of "
                        + spot_dict[int(spots_to_hist[current_spot])]
                        + ' Spot ' + str(spots_to_hist[current_spot]))
    #plt.show()
    fig.savefig('../virago_output/' + chip_name + '/' + chip_name +'_'
                + spot_dict[int(spots_to_hist[current_spot])]
                + '_' + str(spots_to_hist[current_spot]) + '_joyplot.png',
                bbox_inches = 'tight', pad_inches = 0.1, dpi = 300)
    print('File generated: ' + chip_name + '_' + spot_dict[int(spots_to_hist[current_spot])]
                             + '_Spot' + str(spots_to_hist[current_spot]) + '_joyplot.png')
    plt.close()


#*********************************************************************************************#
def density_normalizer(spot_data, pass_counter):
    normalized_density = ([])
    m,p,q = 0,0,0
    for row in spot_data.iterrows():
    # norm_d = spot_data[spot_data['scan_number'] == 1][,'kparticle_density']
    # norm_d2 = spot_data[spot_data['scan_number'] != 1][1:5,'kparticle_density']
        while p < len(spot_data.index):
            if q < (pass_counter) and p < (pass_counter):
                normalized_density.append(round(spot_data.loc[p,'kparticle_density']
                    - spot_data.loc[m,'kparticle_density'],2))
                p += 1
                q += 1
                continue
            elif q == (pass_counter):
                m += (pass_counter)
                normalized_density.append(spot_data.loc[p,'kparticle_density']
                    - spot_data.loc[m,'kparticle_density'])
                p += 1
                q = 1
                continue
            elif q < (pass_counter) and p >= (pass_counter):
                normalized_density.append(round(spot_data.loc[p,'kparticle_density']
                    - spot_data.loc[m,'kparticle_density'],2))
                q += 1
                p += 1
                continue
            else:
                continue
    return normalized_density

    if spot_data.scan_number == 1:
        print("bleh")











#*********************************************************************************************#
#*********************************************************************************************#
def chip_file_reader(xml_file): ##XML file reader, reads the chip file for IRIS experiment
    xml_raw = etree.iterparse(xml_file)
    chip_dict = {}
    chip_file = []
    for action, elem in xml_raw:
        if not elem.text:
            text = "None"
        else:
            text = elem.text
        #print(elem.tag + " => " + text)
        chip_dict[elem.tag] = text
        if elem.tag == "spot":
            chip_file.append(chip_dict)
            chip_dict = {}
    return chip_file
#*********************************************************************************************#
#
#    CODE BEGINS HERE
#
#*********************************************************************************************#
##Point to the correct directory
retval = os.getcwd()
print("\nCurrent working directory is:\n %s" % retval)
iris_path = input("\nPlease type in the path to the folder that contains the IRIS data:\n")
os.chdir(iris_path.strip('"'))
blank = np.array([[1,0,0,0,0,1]])
txt_list = sorted(glob.glob('*.txt'))
pgm_list = sorted(glob.glob("*.pgm"))
csv_list = sorted(glob.glob('*.csv'))
chip_name = pgm_list[0].split(".")[0]

for file in sorted(glob.glob('*/*.xml')):
    chip_file = chip_file_reader(file)
    intro = chip_file[0]
    #chip_name = intro['fittedfilename']
q = 0
spot_dict = {} ##Matches spot antibody type to scan order (spot number)
for dictionary in chip_file:
    test_dict = (chip_file[q])
    spot_dict[q + 1] = test_dict['spottype']
    q += 1

sample_name = input("\nPlease enter a sample descriptor (e.g. VSV-MARV@1E6 PFU/mL)\n")
if not os.path.exists('../virago_output/' + chip_name):
    os.mkdir('../virago_output/' + chip_name)
##Varibles
full_times = pd.DataFrame([])
spot_data_nv = pd.DataFrame([])
averaged_data = []
normalized_density = ([])
spot_labels, spot_set = [], []
hist_dict = {}
#*********************************************************************************************#
# Text file Parser
#*********************************************************************************************#
spot_counter = 0
for file in txt_list:
    txtfile = file
    txtcheck = file.split(".")
    txtdata = pd.read_table(txtfile, sep = ':',
        error_bad_lines = False, header = None,
        index_col = 0, usecols = [0, 1])
    if (len(txtcheck) > 2) and (txtcheck[2].isalpha()):
        print('File scanned:  ' + txtfile)
        real_times, pass_labels = [], []
        spot_counter += 1
        #chip_name = str(txtdata.loc['chip_name'].values).strip(""""[]'""")
        # spot_type = str(txtdata.loc['spot_type'].values).strip(""""[]'""")
        # spot_labels.append(spot_type)
        # if spot_type not in spot_set:
        #     spot_set.append(spot_type)
        pass_labels = [row for row in txtdata.index if row.startswith('pass_time')]
        times_s = pd.Series(txtdata.loc[pass_labels].values.flatten().astype(np.float))
        times_min = round(times_s / 60,2)
        full_times = pd.concat([full_times,times_min], axis = 0, ignore_index = True)
        pass_counter = len(pass_labels) ##This gets used a lot

#*********************************************************************************************#
    elif (len(txtcheck) > 2) and (txtcheck[2].isdigit()):
        print('File scanned:  ' + txtfile)
        scan_number = int(txtcheck[2])
        spot_number = int(txtcheck[1])
        area = float(txtdata.loc['area'])
        solo_spot_data_nv = pd.DataFrame([spot_number, scan_number, area]).T
        spot_data_nv = pd.concat([spot_data_nv, solo_spot_data_nv.rename(columns =
            {0:'spot_number', 1:'scan_number', 2:'area'})],
            axis = 0,
            ignore_index = True)
        txtjoin = ".".join(txtcheck[:-1])
        if any(txtjoin in csv for csv in csv_list):
            continue
            #print("TRUE", txtjoin)
        else:
            print("Missing particle data... Generating blank: ", txtjoin + ".csv")
            np.savetxt(txtjoin + ".csv", blank, delimiter=",", fmt=('%i','%i','%i','%i','%i','%i'))
    else:
        print('Unrecognized text file: ' + txtfile)
spot_labels = [[val]*(pass_counter) for val in spot_dict.values()]
spot_labels2 = pd.Series(np.asarray(spot_labels).flatten())
for val in spot_dict.values():
    if val not in spot_set:
        spot_set.append(val)

spot_data_nv.insert(2, 'scan_time', full_times)
spot_data_nv.insert(2, 'spot_type', spot_labels2)

full_times2 = np.asarray(full_times).reshape((pass_counter,spot_counter))
full_times2 = pd.DataFrame(full_times2)
full_times2.columns = [np.asarray(spot_labels)]
full_times2.index = [pass_labels]



#*********************************************************************************************#
# PGM Scanning
spot,scan = 1,1 ##Change this.......... to only scan certain spots
#*********************************************************************************************#

# for txt in txt_list:
#     txtcheck = txt.split(".")
#     if (txtcheck[2].isdigit()) and any(".".join(txtcheck[:-1]) in pgm for pgm in pgm_list):
#         print("True", txtcheck[:-1])
#     else:
#         print("False", txtcheck[:-1])
if pgm_list:
    pgmer_toggle = input("\nPGM files exist. Do you want scan them? (y/n)\n"
                         + "WARNING: This will take a long time!\n")
    if pgmer_toggle.lower() in ('yes', 'y'):
        startTime = datetime.now()
        pgm_list = sorted(glob.glob('*.pgm'))
        scan_list = []
        spot_data_vir = []

        while spot <= (spot_counter - 0):
            for file in pgm_list:
                pgmfile = file
                pgm_name = pgmfile.split(".")
                scan_no = int(pgm_name[2])
                spot_no = int(pgm_name[1])
                png = '.'.join(pgm_name[:3])
                if (spot_no == spot) & (scan_no == scan):
                    scan_list.append(pgmfile)
                if len(scan_list) == 9:
                    startTime2 = datetime.now()
                    particle_count, uniq_particles, area_sqmm = IRISpgm_scanner(spot, scan,
                                                                                scan_list)
                    scan_data = [spot_no, scan_no, round(area_sqmm,6), particle_count]
                    spot_data_vir.append(scan_data)
                    print(datetime.now() - startTime2)
                    scan_list = []
                    scan += 1
                    if scan > pass_counter:
                        scan = 1
                        spot += 1
        print(datetime.now() - startTime)
        spot_data_vir = pd.DataFrame(spot_data_vir, columns = ('spot_number', 'scan_number',
                                                               'area_sqmm',
                                                               'particle_count_0_0_0_'))
        spot_data_vir.insert(2, 'scan_time', full_times)
        spot_data_vir.insert(2, 'spot_type', spot_labels2)
        for row in spot_data_vir.iterrows():
            kparticle_density = (spot_data_vir.loc[:,'particle_count_0_0_0_']
                                / spot_data_vir.loc[:,'area_sqmm']) / 1000
        spot_data_vir['kparticle_density'] = kparticle_density
        normalized_density_vir = density_normalizer(spot_data_vir, pass_counter)
        spot_data_vir['normalized_density'] = normalized_density_vir
        spot_data_vir.to_csv('../virago_output/' + chip_name + '/'
                             + chip_name + "_spot_data_vir.csv", sep = ",")
        print(spot_data_vir)
    else:
        spot_data_vir = pd.DataFrame([])
#*********************************************************************************************#
# CSV Reader
#*********************************************************************************************#
olddata_toggle = 'no'
if os.path.exists('../virago_output/' + chip_name + '/' + chip_name + '_spot_data_nv.csv'):
    olddata_toggle = input("\nPre-existing data exists. Do you want use this data? (y/n)\n"
        "NOTE: Selecting 'no' will overwrite old data)\t")
    assert isinstance(olddata_toggle, str)
    if olddata_toggle.lower() in ('yes', 'y'):
        ##This reads in the pre-existing data files and stores the needed variables
        print("\nUsing pre-existing data...\n")
        preexist_csv = '../virago_output/' + chip_name + '/' + chip_name + '_spot_data_nv.csv'
        spot_data_nv = pd.read_table(preexist_csv, sep = ',', error_bad_lines = False)
        count_info = [col for col in spot_data_nv.columns if col.startswith('particle_count')]
        count_info = str(count_info).split("_")
        min_corr = float(count_info[2])
        contrast_window = count_info[3:5]
        print(str(contrast_window[0])+"-"+contrast_window[1])
        min_corr_str = str("%.2F" % min_corr)
        print("Correlation value is: "+ min_corr_str +" or greater\n")
        with open('../virago_output/' + chip_name + '/'
            + chip_name + '_particle_dict_'
            + min_corr_str + 'corr.txt', 'r') as infile:
            particle_dict = json.load(infile)
    elif olddata_toggle.lower() in ('no', 'n'): ##See IRIScsv_read.py for deets
        print("")
        min_corr, spot_data_nv, particle_dict, contrast_window = nano_csv_reader(chip_name, spot_data_nv)
else:

    min_corr, spot_data_nv, particle_dict, contrast_window = nano_csv_reader(chip_name, spot_data_nv)
min_corr_str = str("%.2F" % min_corr)





uniq_part_list = sorted(glob.glob('../virago_output/' + chip_name + '/' + '*uniq_particles.csv'))
pgm_set = set(glob.glob('*.pgm')) - set(glob.glob('*0.pgm'))
if len(uniq_part_list) == (len(pgm_set) / 9):
    csv_reader_toggle = input("VIRAGO has scanned these images before. Use this data? (y/n)\n")
    assert isinstance(csv_reader_toggle, str)
    if csv_reader_toggle in ('yes', 'y'):
        spot_data_vir, contrast_window, particle_dict = virago_csv_reader(chip_name, spot_data_vir)



if (olddata_toggle.lower() not in ('yes', 'y')):
    nv_vir_toggle = input("Would you like to use nanoViewer or VIRAGO data? (type N or V)\n")
    assert isinstance(nv_vir_toggle, str)
    if nv_vir_toggle.lower() in ('n','nanoViewer'):
        print("Using nanoViewer data...")
        spot_data = spot_data_nv
    else:
        print("Using VIRAGO data...")
        spot_data = spot_data_vir
else:
    spot_data = spot_data_vir

spot_data.kparticle_density.replace(to_replace = 0, value = np.nan, inplace = True)
# -------------------------------------------------------------------
#####################################################################
# Joyplot generator
#####################################################################
#--------------------------------------------------------------------
# if int(sys.version[0]) == 3:
#     joyplot(min_corr, chip_name, spot_dict, contrast_window)
# -------------------------------------------------------------------
#####################################################################
# Histogram generator
#####################################################################
#--------------------------------------------------------------------

spots_to_hist = input("Which spots would you like to generate histograms for?\t")
spots_to_hist = spots_to_hist.split(",")
print(spots_to_hist)
cont_0 = float(contrast_window[0])
cont_1 = float(contrast_window[1])
for spot in spots_to_hist:
    hist_dict = {}
    for key in sorted(particle_dict.keys()):
        hist_spot = int(key.split(".")[0])
        if hist_spot == int(spot):
            hist_dict[key] = particle_dict[key]
    nrows = 2
    ncols = math.ceil(pass_counter / 2)
    fig = plt.figure()
    plt.axis('off')
    #fig.xlabel("Percent Contrast", size = 15)
    # ax = plt.axes([0,0,1,1])
    # #ax.set_yticks([0.5])
    fig.text(0.06,0.6,"Particle Counts >= " + min_corr_str,
             fontsize = 10, rotation = 'vertical')
    fig.text(.4,0.04,"Percent Contrast", fontsize = 10)
    #plt.gca().axison = False
    # plt.title(plot_title, size = 12)

    for key in sorted(hist_dict.keys()):
        hist_pass = int(key.split(".")[1])
        sbplt = fig.add_subplot(nrows,ncols,hist_pass)
        (hist_vals, bins, patches) = sbplt.hist([hist_dict[key]],
                                     100, range = [cont_0,cont_1], color = ['#0088FF'],
                                     rwidth = 1, alpha = 0.75, normed = False)
        plt.xticks(np.arange(cont_0, cont_1+1,2), size = 4)
        if max(hist_vals) <= 50:
            grads = 5
        elif max(hist_vals) <= 50:
            grads = 10
        else:
            grads = 25
        plt.yticks(np.arange(0, (max(hist_vals)) + 10 , grads), size = 4)

        # if hist_pass == 1:
        #     plt.ylabel("Count", size = 10)
        # if hist_pass == (math.floor(hist_pass / 2) + 1):
        #     plt.ylabel("Particle", size = 10)
        plt.title("Pass "+ str(hist_pass), size = 4)
        plt.grid(True, alpha = 0.5)

        sbplt.set_ylim(0, (max(hist_vals)) + 10)
    plot_title = str("Particle Contrast Distribution of " + chip_name + " "
                    + spot_labels[int(spot)-1][0]
                    + " Spot " + spot)
    plt.suptitle(plot_title, size = 12)
    plt.subplots_adjust(wspace = 0.25)
        #bins = np.delete(bins, bins[0])
        #plt.text(7.5, 175,"Scan " + str(hist_pass), va = 'baseline', size = 8)
        # mu = np.sum(np.multiply(hist_vals, bins))/np.sum(hist_vals)
        # var = np.sum(np.multiply((hist_vals - mu)**2, bins)) / np.sum(hist_vals)
        # sigma = (var)**(0.125)

        #plt.axvline(mu,color = 'b',linewidth = 1, linestyle = '--')
        #y = mlab.normpdf(bins, mu, sigma)
        #plt.plot(bins, y, 'r--', linewidth = 1)
    #plt.show()
    #plt.subplots_adjust(left=0.15)
    plt.savefig('../virago_output/' + chip_name + '/' + chip_name + '_spot-' + spot
                +  '_histo.png', bbox_inches = 'tight', dpi = 300)
    print('File generated: ' +  chip_name + '_spot-' + spot + '_histo.png')
    plt.close()

# -------------------------------------------------------------------
#####################################################################
# Particle count normalizer so pass 1 = 0 particle density
#####################################################################
#--------------------------------------------------------------------
m,p,q = 0,0,0
spot_data2 = spot_data
spot_data2.kparticle_density.fillna(value = 0, inplace = True)
for row in spot_data2.iterrows():
    while p < len(spot_data2.index):
        if q < (pass_counter) and p < (pass_counter):
            normalized_density.append(round(spot_data2.loc[p,'kparticle_density']
                - spot_data2.loc[m,'kparticle_density'],2))
            p += 1
            q += 1
            continue
        elif q == (pass_counter):
            m += (pass_counter)
            normalized_density.append(spot_data2.loc[p,'kparticle_density']
                - spot_data2.loc[m,'kparticle_density'])
            p += 1
            q = 1
            continue
        elif q < (pass_counter) and p >= (pass_counter):
            normalized_density.append(round(spot_data2.loc[p,'kparticle_density']
                - spot_data2.loc[m,'kparticle_density'],2))
            q += 1
            p += 1
            continue
        else:
            continue
spot_data['normalized_density'] = normalized_density
# -------------------------------------------------------------------
#####################################################################
# This gets the average values and standard deviations for each spot type
#####################################################################
#--------------------------------------------------------------------
k = 0
for val in spot_set:
    x = 1
    for val in pass_labels:
        data_slice = spot_data[['spot_type', 'scan_time', 'kparticle_density',
                                'normalized_density']][(spot_data['scan_number'] == x)
                                & (spot_data['spot_type'] == spot_set[k])]
        scan_time_mean = round(data_slice['scan_time'].mean(),2)
        filt_density_mean = round(data_slice['kparticle_density'].mean(),2)
        filt_density_std = round(np.std(data_slice['kparticle_density']),2)
        norm_density_mean = round(data_slice['normalized_density'].mean(),2)
        norm_density_std = round(np.std(data_slice['normalized_density']),4)
        avg_data = (spot_set[k],
            spot_data.loc[x - 1,'scan_number'],
            scan_time_mean,
            filt_density_mean,
            filt_density_std,
            norm_density_mean,
            norm_density_std)
        averaged_data.append(avg_data)
        x += 1
    k += 1
averaged_data = pd.DataFrame(averaged_data,
                             columns =  ['spot_type', 'scan_number', 'avg_scan_time',
                                         'avg_kparticle_density', 'kparticle_density_std',
                                         'avg_normalized_density', 'normalized_density_std']
                             )
# -------------------------------------------------------------------
#####################################################################
# Asks whether the time series should be set such that Time 0 == 0 particle density
#####################################################################
#--------------------------------------------------------------------
baseline_toggle = input("Do you want the time series chart normalized to baseline? (y/n)\t")
assert isinstance(baseline_toggle, str)
if baseline_toggle.lower() in ('yes', 'y'):
    filt_toggle = 'normalized_density'
    avg_filt_toggle = 'avg_normalized_density'
    stdev_filt_toggle = 'normalized_density_std'
    print("Normalizing...")
else:
    filt_toggle = 'kparticle_density'
    avg_filt_toggle = 'avg_kparticle_density'
    stdev_filt_toggle = 'kparticle_density_std'
# -------------------------------------------------------------------
#####################################################################
# Time Series Generator
#####################################################################
#--------------------------------------------------------------------
colormap = ('#e41a1c','#377eb8','#4daf4a',
            '#984ea3','#ff7f00','#ffff33',
            '#a65628','#f781bf','gray','black')
fig = plt.figure(figsize = (8,6))
subplot = fig.add_subplot(111)
n,c = 0,0
for val in np.arange(1,spot_counter + 1):
    time_x = spot_data[spot_data['spot_number'] == val]['scan_time'].reset_index(drop = True)
    density_y = spot_data[spot_data['spot_number'] == val][filt_toggle].reset_index(drop = True)
    while n > 1:
        if spot_labels[n-1] != spot_labels[n]:
            c += 1
            break
        else:
            break
    subplot.plot(time_x, density_y, marker = '+', linewidth = 1,
                 color = colormap[c], alpha = 0.4, label = '_nolegend_')
    n += 1
n = 0
for spot in spot_set:
    avg_data = averaged_data[averaged_data['spot_type'].str.contains(spot)]
    avg_time_x = avg_data['avg_scan_time']
    avg_density_y = avg_data[avg_filt_toggle]
    errorbar_y = avg_data[stdev_filt_toggle]
    subplot.errorbar(avg_time_x, avg_density_y,
                    yerr = errorbar_y, marker = 'o', label = spot_set[n],
                    linewidth = 2, elinewidth = 1, capsize = 3,
                    color = colormap[n], alpha = 0.9, aa = True)
    n += 1
#min_corr_str = str(0)
plt.xlabel("Time (min)", color = 'gray')
plt.ylabel('Particle Density (kparticles/sq. mm)\n'+ contrast_window[0]+'-'+contrast_window[1]
            + '% Contrast, Correlation Value >=' + min_corr_str, color = 'gray')
plt.xticks(np.arange(0, max(time_x)+1, 2), color = 'gray')
plt.yticks(color = 'gray')
plt.title(chip_name + ' Time Series of ' + sample_name)
plt.legend(loc = 'upper left', fontsize = 10, ncol = 1)
plt.axhline(linestyle = '--', color = 'gray')
plot_name = chip_name + '_timeseries_' + min_corr_str + 'corr.png'

plt.savefig('../virago_output/' + chip_name + '/' +  plot_name,
            bbox_inches = 'tight', pad_inches = 0.1, dpi = 300)
print('File generated: ' + plot_name)
csv_spot_data = str('../virago_output/' + chip_name + '/' + chip_name + '_spot_data.csv')
spot_data.to_csv(csv_spot_data, sep = ',')
#plt.show()
plt.clf(); plt.close('all')
print('File generated: '+ csv_spot_data)
# -------------------------------------------------------------------
#####################################################################
# Bar Plot Generator
#####################################################################
#--------------------------------------------------------------------


last_scan = np.max(spot_data['scan_number'])
baseline = (spot_data[spot_data['scan_number'] == 2][['spot_type', 'kparticle_density']]).reset_index(drop = True)
post_scan = pd.Series(spot_data[spot_data['scan_number'] == last_scan]['kparticle_density'],
                      name = 'post_scan').reset_index(drop = True)
difference = pd.Series(spot_data[spot_data['scan_number'] == last_scan]['normalized_density'],
                    name = 'difference').reset_index(drop = True)
barplot_data = pd.concat([baseline, post_scan, difference], axis = 1)
#barplot_data.kparticle_density = barplot_data.kparticle_density * -1
baseline_avg, post_scan_avg, baseline_std, post_scan_std, diff_avg, diff_std = [],[],[],[],[],[]
for spot in spot_set:
    avg_data = barplot_data[barplot_data['spot_type'].str.contains(spot)]
    baseline_avg.append(np.mean(avg_data.kparticle_density))
    baseline_std.append(np.std(avg_data.kparticle_density))

    post_scan_avg.append(np.mean(avg_data.post_scan))
    post_scan_std.append(np.std(avg_data.post_scan))

    diff_avg.append(np.mean(avg_data.difference))
    diff_std.append(np.std(avg_data.difference))
fig,axes = plt.subplots(nrows = 1, ncols = 2, figsize = (9,4), sharey = True)
fig.subplots_adjust(left=0.08, right=0.98, wspace=0)
plt.suptitle(chip_name + " - Initial and Final Scans\n" + "Sample Conditions: " + sample_name)

bar1 = axes[0].bar(np.arange(len(spot_set)), baseline_avg, width = 0.45, color = colormap[0],
                   tick_label = spot_set, yerr = baseline_std,capsize = 4, alpha = 0.75)

bar2 = axes[0].bar(np.arange(len(spot_set)) + 0.45, post_scan_avg, width = 0.45,
                   color = colormap[1], tick_label = spot_set, yerr = post_scan_std,
                   capsize = 4, alpha = 0.75)

axes[0].set_ylabel('Particle Density (kparticles/sq. mm)\n' + contrast_window[0]
            +'-'+contrast_window[1] + '% Contrast, Correlation Value >='
            + min_corr_str, color = 'gray', size = 10)
bar3 = axes[1].bar(np.arange(len(spot_set)) + (0.45/2), diff_avg, width = 0.5,
                   color = colormap[3],tick_label = spot_set, yerr = diff_std, capsize = 4)
for ax in axes:
    ax.yaxis.grid(True)
    ax.set_xticks(np.arange(len(spot_set)) + (0.45/2))
    ax.set_xlabel("Antibody", color = 'gray', size = 10)

plt.figlegend((bar1,bar2,bar3),
              ('Initial Scan', 'Final Scan', 'Difference'),
              loc = 'upper right', fontsize = 10)
#plt.subplots_adjust(left=0.15)
plt.savefig('../virago_output/' + chip_name + '/' + chip_name + '_'
            + contrast_window[0] + '-' + contrast_window[1] + '_'
            +  min_corr_str + '_barplot.png',
            bbox_inches = 'tight', pad_inches = 0.1, dpi = 300)
#plt.show()
plt.clf(); plt.close('all')
