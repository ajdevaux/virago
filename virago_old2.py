#! /usr/bin/env python
from future.builtins import input
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import pandas as pd
import numpy as np
from scipy import stats
import glob, os, json
#from builtins import input
pd.set_option('display.width', 1000)

retval = os.getcwd()
print("Current working directory %s" % retval)

os.chdir('../DATA/IRIS/pCHIP_results/pCHIP001')
if not os.path.exists('../virago_output'):
    os.mkdir('../virago_output')
full_times = pd.DataFrame([])
spot_data = pd.DataFrame([])
averaged_data = []
filtered_counts, filtered_density2, normalized_density, density_y_combined = ([]), ([]), ([]),([])
spot_labels, spot_labels2, spot_labels3 = [], [], []
spot_set = []
particle_dict, spot_dict = {}, {}

for file in glob.glob('*.txt'): ##Text file parser
    txtfile = file
    print('File scanned:  '+ txtfile)
    data = pd.read_table(txtfile, sep = ':', error_bad_lines = False, names = ("Parameter", "Value"), index_col = 0)
    #print(data)
    if 'chip_name' in data.index: ##This parses text in text files generated by IRIS instrument
        real_times, pass_labels = [], []
        pass_counter = 1
        spot_type = pd.Series(data.loc['spot_type'])
        spot_type = str(spot_type.values)
        spot_type = spot_type[2:-2]
        spot_labels.append(spot_type)
        if spot_type not in spot_set:
            spot_set.append(spot_type)
        for row in data.iterrows():
            if pass_counter < 10:
                pass_time = 'pass_time00' + str(pass_counter)
                if pass_time not in data.index:
                    break
                else:
                    time_sec = data.loc[pass_time]
                    time_min = round(float(time_sec) / 60, 2)
                    real_times.append(time_min)
                    pass_labels.append(pass_time)
                    pass_counter += 1
            elif 10 <= pass_counter < 100:
                pass_time = 'pass_time0' + str(pass_counter)
                if pass_time not in data.index:
                    break
                else:
                    time_sec = data.loc[pass_time]
                    time_min = round(float(time_sec) / 60, 2)
                    real_times.append(time_min)
                    pass_labels.append(pass_time)
                    pass_counter += 1
    elif 'data_file' in data.index: ##This parses text files generated by nanoViewer software
        data_file = str(data.loc['data_file'].values)
        scan_number = int(data_file[15:-2])
        spot_number = int(data_file[11:-6])
        area = float(data.loc['area'])
        solo_spot_data = pd.DataFrame([spot_number, scan_number, area]).T
        spot_data = pd.concat([spot_data, solo_spot_data.rename(columns = {0: 'spot_number', 1:'scan_number',2:'area'})], axis = 0, ignore_index = True)
        continue
    else:
        continue
    real_times = pd.Series(real_times)
    full_times = pd.concat([full_times,real_times], axis = 0, ignore_index = True)

for val in spot_labels:
    spot_labels2.extend([[val]*(pass_counter-1)])
for sublist in spot_labels2:
    for val in sublist:
        spot_labels3.append(val)
spot_data.insert(2, 'scan_time', full_times)
spot_data.insert(2, 'spot_type', spot_labels3)
full_times = np.asarray(full_times).reshape((10,15))
full_times = pd.DataFrame(full_times)
full_times.columns = [np.asarray(spot_labels)]
full_times.index = [pass_labels]

chip_name = data.loc['chip_name']
chip_name = str(chip_name.values)
chip_name = chip_name[2:-2]
all_passes = range(0, pass_counter - 1)


if os.path.exists('../virago_output/spot_data_' + chip_name + '.csv'):
    olddata_yesno = input("\nPre-existing data exits. Do you want use this data?(y/n)\n(NOTE: Selecting 'no' will overwrite old data)\t")
    assert isinstance(olddata_yesno, str)
    if olddata_yesno in ('no', 'n'):
        min_corr = float(input("What is the correlation cutoff for particle count? (choose value between 0.5 and 1)\t"))
        if min_corr > 1 and len(str(min_corr)) == 1:
            min_corr = min_corr / 10
            print(min_corr)
        elif min_corr > 1 and len(str(min_corr)) == 2:
            min_corr = min_corr / 100
            print(min_corr)

        for file in glob.glob('*.csv'): ##This pulls particle data from the CSVs generated by nanoViewer
            csvfile = file
            data2 = pd.read_table(csvfile, sep = ',', error_bad_lines = False, usecols = [1,2,3,4,5], names = ("Contrast", "Correlation", "X", "Y", "Slice"))
            #print(data2)
            k, particles = 0,0
            corr2 = pd.DataFrame([0,0])
            for row in data2.iterrows():
                contrast = data2.get_value(k, 'Contrast')
                contrast = (contrast - 1) * 100
                corr = data2.get_value(k, 'Correlation')
                if 0 < contrast <= 10 and corr >= min_corr:
                    filter_df = pd.DataFrame([contrast, corr]).T
                    corr2 = pd.concat([corr2, filter_df], ignore_index = True)
                    k += 1
                    particles += 1
                    continue
                else:
                    k += 1
                    continue
            csv_id = csvfile[9:-4]
            particle_dict[csv_id] = list(corr2.iloc[:,0])
            filtered_counts.append(particles)
            print('File scanned:  '+ csvfile + '; Particles counted: ' + str(particles))
        spot_data['filtered_count_'+ str(min_corr)] = filtered_counts
        ##Generates density data from filtered counts and area values
        for row in spot_data.iterrows():
            filtered_density = (spot_data.loc[:,'filtered_count_' + str(min_corr)] / spot_data.loc[:,'area'])/1000
        spot_data = pd.concat([spot_data, filtered_density.rename('filtered_density')], axis = 1)
        with open('../virago_output/particle_dict' + str(min_corr) + '.txt', 'w') as outfile:
            json.dump(particle_dict, outfile, indent = 4)

    elif olddata_yesno in ('yes', 'y'): ##This reads in the pre-existing data files and stores the needed variables
        print("\nUsing pre-existing data...\n")
        preexist_csv = '../virago_output/spot_data_' + chip_name + '.csv'
        spot_data = pd.read_table(preexist_csv, sep = ',', error_bad_lines = False)
        min_corr = [col for col in spot_data.columns if col.startswith("filtered_count")]
        min_corr = min_corr[0]
        min_corr = float(min_corr[-4:])
        with open('../virago_output/particle_dict' + str(min_corr) + '.txt', 'r') as infile:
            particle_dict = json.load(infile)
print(particle_dict)
##Histogram generator for each CSV dataset
spots_to_hist = input("Which spots would you like to generate histograms for?\t")
for key in sorted(particle_dict.keys()):
    if int(key[:3]) == int(spots_to_hist):
        fig = plt.figure(figsize = (6,4))
        subplot = fig.add_subplot(111)
        subplot.hist([particle_dict[key]], 100, range = [0,10], color = ['#0088FF'], rwidth = 1, alpha = 0.75)
        plt.xlabel("Percent Contrast")
        plt.grid(True)
        plt.ylabel("Particle Count\n Correlation Value >= " + str(min_corr))
        plot_title = str("Particle Contrast Distribution of " + chip_name + '_' + key[:7])
        plt.title(plot_title)
        plt.savefig('../virago_output/' +  chip_name + '_' + key[:7] + '-histo.svg', bbox_inches = 'tight', pad_inches = 0.1, dpi = 300)
        print('File generated: ' +  chip_name + '_' + key[:7] + '-histo.svg')
        plt.show()
        plt.clf()
        continue
plt.close()

##Normalizes the particle counts so pass 1 = 0 particle density
m,p,q = 0,0,0
for row in spot_data.iterrows():
    while p < len(spot_data.index):
        if q < (pass_counter-1) and p < (pass_counter-1):
            normalized_density.append(round(spot_data.loc[p,'filtered_density'] - spot_data.loc[m,'filtered_density'],2))
            p += 1
            q += 1
            continue
        elif q == (pass_counter - 1):
            m += (pass_counter-1)
            normalized_density.append(spot_data.loc[p,'filtered_density'] - spot_data.loc[m,'filtered_density'])
            p += 1
            q = 1
            continue
        elif q < (pass_counter-1) and p >= (pass_counter-1):
            normalized_density.append(round(spot_data.loc[p,'filtered_density'] - spot_data.loc[m,'filtered_density'],2))
            q += 1
            p += 1
            continue
        else:
            continue
spot_data['normalized_density'] = normalized_density

k = 0
for val in spot_set:
    x = 1
    for val in all_passes:
        data_slice = spot_data[['spot_type','scan_time','filtered_density','normalized_density']][(spot_data['scan_number'] == x) & (spot_data['spot_type'] == spot_set[k])]
        scan_time_mean = round(data_slice['scan_time'].mean(),2)
        filt_density_mean = round(data_slice['filtered_density'].mean(),2)
        filt_density_std = round(np.std(data_slice['filtered_density']),2)
        norm_density_mean = round(data_slice['normalized_density'].mean(),2)
        norm_density_std = round(np.std(data_slice['normalized_density']),4)
        avg_data = (spot_set[k], spot_data.loc[x-1,'scan_number'],scan_time_mean, filt_density_mean, filt_density_std, norm_density_mean, norm_density_std)
        averaged_data.append(avg_data)
        x += 1
    k += 1
averaged_data = pd.DataFrame(averaged_data, columns = ['spot_type', 'scan_number', 'avg_scan_time', 'avg_filtered_density', 'filtered_density_std','avg_normalized_density', 'normalized_density_std'])

##Time data plot
baseline_yesno = input("Do you want the time series chart normalized to baseline? (y/n)\t")
assert isinstance(baseline_yesno, str)
if baseline_yesno in ('yes', 'y'):
    filt_norm = 'normalized_density'
    avg_filt_norm = 'avg_normalized_density'
    stdev_filt_norm = 'normalized_density_std'
    print("Normalizing...")
else:
    filt_norm = 'filtered_density'
    avg_filt_norm = 'avg_filtered_density'
    stdev_filt_norm = 'filtered_density_std'

colormap = ('#e41a1c','#377eb8','#4daf4a','#984ea3','#ff7f00','#ffff33','#a65628','#f781bf')

fig = plt.figure(figsize = (8,6))
subplot = fig.add_subplot(111)
n,c = 0,0
for val in spot_labels:
    time_x = np.asarray(spot_data.loc[n*(pass_counter-1):(n*(pass_counter-1))+(pass_counter-2),'scan_time'])
    density_y = np.asarray(spot_data.loc[n*(pass_counter-1):(n*(pass_counter-1))+(pass_counter-2),filt_norm])
    while n > 1:
        if full_times.columns[n-1] == full_times.columns[n]:
            break
        elif full_times.columns[n-1] != full_times.columns[n]:
            c += 1
            break
    subplot.plot(time_x, density_y, marker = '+', linewidth = 1, color = colormap[c], alpha = 0.33)
    n += 1
n,c = 0,0
for val in spot_set:
    avg_time_x = np.asarray(averaged_data.loc[n*(pass_counter-1):(n*(pass_counter-1))+(pass_counter-2),'avg_scan_time'])
    avg_density_y = np.asarray(averaged_data.loc[n*(pass_counter-1):(n*(pass_counter-1))+(pass_counter-2),avg_filt_norm])
    errorbar_y = np.asarray(averaged_data.loc[n*(pass_counter-1):(n*(pass_counter-1))+(pass_counter-2),stdev_filt_norm])
    subplot.errorbar(avg_time_x, avg_density_y, yerr = errorbar_y, marker = 'o', label = spot_set[n], linewidth = 2, elinewidth = 1, capsize = 3, color = colormap[n], alpha = 0.9, aa = True)
    n += 1
plt.xlabel("Time (min)", color = 'gray')
plt.ylabel("Particle Density (kparticles/sq. mm)\n Correlation Value >= " + str(min_corr), color = 'gray')
plt.xticks(np.arange(0, max(time_x)+1, 2), color = 'gray')
plt.yticks(color = 'gray')
plt.title(chip_name + ' Time Series of VSV-MARV@1E6 PFU/mL')
plt.legend(loc = 'upper left', fontsize = 10, ncol = 1)
plt.axhline(linestyle = '--', color = 'gray')
plot_name = chip_name + '_timeseries_' + str(min_corr) + 'corr.svg'
plt.show()
plt.savefig('../virago_output/' +  plot_name, bbox_inches = 'tight', pad_inches = 0.1, dpi = 300)
print('File generated: ' + plot_name)
plt.clf(); plt.close('all')
spot_data.to_csv('../virago_output/spot_data_' + chip_name + '.csv', sep = ',')
print('File generated: spot_data_' + chip_name + '.csv')
